[
  {
    "path": "posts/2025-09-20-nerf-accuracy/",
    "title": "Nerf blaster accuracy",
    "description": "More geometry and Stan, this time with Nerf blasters",
    "author": [
      {
        "name": "James Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2025-09-21",
    "categories": [],
    "contents": "\r\nDuring COVID lockdown, the kids (and their dad, for that matter) were a bit obsessed with Nerf blasters. Since dad is also a stats nerd, it didn’t take too long until we started collecting data on this. So I built a target out of some paper and cardboard, and we went down to the basement with some blasters and darts. What follows is me actually getting to analyze the data, 5 years on.\r\nIf you’re interested in this kind of stuff, you may also be interested in my other posts on NFL field goal accuracy and Australian football set shot accuracy, which were inspired by Andrew Gelman’s golf putting model.\r\nThe data-generating process\r\nThe target\r\nI wanted to have data that was more informative than just “hit” and “miss”, so I built a target with nine regions. On one side of the target each region was a letter-sized piece of paper, and on the other side of the target each region was a letter sized piece of paper folded in half. I made these two targets because I wanted to collect data for both short- and long-ragne shots. Having the larger-sized target turned out to be a good one to have for shooting from a range of 20 feet, and the smaller-sized target was better for shooting from ten feet (our basement had 2-foot floor tiles, so this was very easy to measure). By this, I mean that for these ranges, it turned out that each target had a very good chance of actually being hit, but there was a also good spread of hits in different regions. Here is what the target looked like:\r\n\r\n\r\n\r\nIn the data, I have the region of the target that was hit, 0-9, with 0 indicating the entire target was missed.\r\nObservable heterogeneity\r\nApart from varying the distance and target size, the data can be sliced three different ways based on (i) who was shooting, (ii) which blaster was being used, and (iii) which darts were being used.\r\nFirstly, we had three people generating the data:\r\n\r\n\r\nDsum<-\"../../data/NERF.csv\" |>\r\n  read.csv() |>\r\n  filter(Blaster!=\"\") |>\r\n  pivot_longer(\r\n    cols = X0:X9\r\n  ) |>\r\n  group_by(Blaster,Darts,Person) |>\r\n  summarize(\r\n    shots = sum(value)\r\n  )\r\n\r\nDsum |> \r\n  group_by(Person) |>\r\n  summarize(\r\n    shots=sum(shots)\r\n  ) |>\r\n  kbl() |>\r\n  kable_classic(full_width=FALSE)\r\n\r\n\r\nPerson\r\n\r\n\r\nshots\r\n\r\n\r\nA\r\n\r\n\r\n62\r\n\r\n\r\nD\r\n\r\n\r\n181\r\n\r\n\r\nJ\r\n\r\n\r\n98\r\n\r\n\r\nHere “A” and “J” are my kids (at the time aged 9 and 5, respectively), and I am “D” for Dad. Yes, I may have gone down to the basement by myself a few times in the interest of statistical rigor.\r\nNext, we had five different blasters to shoot at the target:\r\n\r\n\r\nDsum |> \r\n  group_by(Blaster) |>\r\n  summarize(\r\n    shots=sum(shots)\r\n  ) |>\r\n  kbl() |>\r\n  kable_classic(full_width=FALSE)\r\n\r\n\r\nBlaster\r\n\r\n\r\nshots\r\n\r\n\r\nAPP\r\n\r\n\r\n18\r\n\r\n\r\nNinja\r\n\r\n\r\n80\r\n\r\n\r\nSA\r\n\r\n\r\n117\r\n\r\n\r\nSF\r\n\r\n\r\n20\r\n\r\n\r\nSW\r\n\r\n\r\n106\r\n\r\n\r\nThese were:\r\nXShot Ninja No Rez Foam Dart Blaster, recorded as “Ninja” in the data.\r\nNerf Star Wars Galaxy’s Edge First Order Stormtrooper Nerf Blaster, recorded as “SW” in the data\r\nNerf N-Strike SharpFire Blaster, recorded as “SF” in the data. We used this with all of the extensions removed (stock and barrel)\r\nNerf N Strike Elite Strongarm, recorded as “SA” in the data.\r\nWhatever “APP” was (I forget which one this is)\r\nFinally, we had three different kinds of darts:\r\n\r\n\r\nDsum |> \r\n  group_by(Darts) |>\r\n  summarize(\r\n    shots=sum(shots)\r\n  ) |>\r\n  kbl() |>\r\n  kable_classic(full_width=FALSE)\r\n\r\n\r\nDarts\r\n\r\n\r\nshots\r\n\r\n\r\nAS\r\n\r\n\r\n225\r\n\r\n\r\nLV\r\n\r\n\r\n98\r\n\r\n\r\nX\r\n\r\n\r\n18\r\n\r\n\r\nThese were:\r\nLittle Valentine knock-offs for Nerf N-Strike Elite darts (foam). These are recorded as “LV” in the data.\r\nNerf Accustrike darts. These are the same shape as the other kind, but have a fancy tip that Nerf claims increases their accuracy. This is a testable assumption with our data. These are recorded as “AS” in the data.\r\nWhatever “X” was. I can’t remember what these were, but looking at the data they were only ever used with the “APP” blaster (and the APP blaster was only ever used with the X darts).\r\nI will start by ignoring all three of these dimensions of heterogeneity, but will add them in after the first pooled estimation.\r\nStatistical model\r\nThis is going to be another angle accuracy model, so we need expressions for the angles that one can shoot through in order to hit each region of the target. Here is a picture of the problem:\r\n\r\n\r\n\r\nLet \\(w\\) be the length of the piece of paper, and \\(x\\) be the distance to the target, then:\r\n\\[\r\n\\begin{aligned}\r\n\\tan\\left(\\frac{\\theta}{2}\\right)&=\\frac{w}{2x}\\\\\r\n\\frac{\\theta}{2}&=\\tan^{-1}\\left(\\frac{w}{2x}\\right)\\\\\r\n\\tan\\left(\\frac{\\theta}{2}+\\gamma\\right)&=\\frac{3w}{2x}\\\\\r\n\\gamma&=\\tan^{-1}\\left(\\frac{3w}{2x}\\right)-\\frac{\\theta}{2}\r\n\\end{aligned}\r\n\\]\r\nSo if you hit the center of the target, you must have shot at an angle of between \\(-\\frac{\\theta}{2}\\) and \\(+\\frac{\\theta}{2}\\) radians; if you hit the top part of the target, you must have shot at an angle between \\(\\frac{\\theta}{2}\\) and \\(\\frac{\\theta}{2}+\\gamma\\) radians; and so on.\r\nOnce we have these angles, we can assume a distribution for the angle accuracy. I chose a Student’s-\\(t\\) distribution centered on zero for this. It would have made things simpler to use a normal, but based on my experience with Australian football set shots, the \\(t\\) distribution is much better at modeling data with more than two outcomes (and the Australian football problem is just the Nerf darts problem in two dimension rather than three). Therefore the probability of hitting (say) the middle of the target is:\r\n\\[\r\nF_t\\left(\\frac{\\theta}{2}\\mid 0,\\nu,\\sigma\\right)-F_t\\left(-\\frac{\\theta}{2}\\mid 0,\\nu,\\sigma\\right)\r\n\\]\r\nAnd the probability of hitting (say) one of the outer regions of the target is:\r\n\\[\r\nF_t\\left(\\frac{\\theta}{2}+\\gamma\\mid 0,\\nu,\\sigma\\right)-F_t\\left(\\frac{\\theta}{2}\\mid 0,\\nu,\\sigma\\right)\r\n\\]\r\nI assume that the vertical accuracy is independent of the horizontal accuracy, and so we can just multiply these probabilities together. I assume that \\(\\sigma\\) and \\(\\nu\\) are different for horizontal and vertical angle accuracy, so the model has four parameters.\r\nPooled model\r\nFirst, I estimate a model treating each kind of dart, blaster, and person as equal. Here is the Stan program I wrote to do this:\r\n\r\n\r\ndata {\r\n  int<lower=0> N; // number of rows of data\r\n  vector[N] count; // count of the number of times that outcome occurred\r\n  vector[N] angle_x_lb; // lower bound for horizontal angle\r\n  vector[N] angle_x_ub; // upper bound of horizontal angle\r\n  vector[N] angle_y_lb; // lower bound of vertical angle\r\n  vector[N] angle_y_ub; // upper bound of vertical angle\r\n  vector[N] miss; // =1 if the netire target was missed, zero otherwise\r\n  \r\n  vector[2] whLetter; // dimensions of a letter-sized piece of paper, in meters. Long side is element 1\r\n  \r\n  // priors for parameters, both log-normal\r\n  vector[2] prior_sigma;\r\n  vector[2] prior_nu;\r\n  \r\n  // indicator to use the data or not (useful for prior calibration)\r\n  int<lower=0,upper=1> UseData;\r\n}\r\n\r\n\r\nparameters {\r\n  \r\n  real<lower=0> sigma_x; // scale in horizontal direction\r\n  real<lower=0> nu_x; // df in horizontal direction\r\n  \r\n  real<lower=0> sigma_y; // scale in vertical direction\r\n  real<lower=0> nu_y; // df in vertical direction\r\n  \r\n}\r\n\r\ntransformed parameters {\r\n  \r\n  /* Compute log probability of each outcome. I did this in the transformed parameters \r\n  block because I was debugging the code and wanted to see what the predictions were.\r\n  */\r\n   vector[N] lpr;\r\n    for (ii in 1:N) {\r\n     lpr[ii] = log(student_t_cdf(angle_x_ub[ii], nu_x, 0.0, sigma_x)-student_t_cdf(angle_x_lb[ii], nu_x, 0.0, sigma_x))\r\n                    +\r\n                    log(student_t_cdf(angle_y_ub[ii], nu_y, 0.0, sigma_y)-student_t_cdf(angle_y_lb[ii], nu_y, 0.0, sigma_y))\r\n                    ;\r\n    }                \r\n    // update for misses\r\n      lpr = lpr.*(1.0-miss)+log(1.0-exp(lpr)).*miss;\r\n      \r\n      vector[N] prob = exp(lpr);\r\n  \r\n  \r\n}\r\n\r\n\r\nmodel {\r\n  \r\n  // if we are using the data, increment the likelihood\r\n  if (UseData==1) {\r\n      target += count.*lpr;\r\n  }\r\n  // priors\r\n  target += lognormal_lpdf(sigma_x | prior_sigma[1],prior_sigma[2]);\r\n  target += lognormal_lpdf(sigma_y | prior_sigma[1],prior_sigma[2]);\r\n  target += lognormal_lpdf(nu_x | prior_nu[1],prior_nu[2]);\r\n  target += lognormal_lpdf(nu_y | prior_nu[1],prior_nu[2]);\r\n  \r\n}\r\n\r\ngenerated quantities {\r\n  \r\n  /* Make some predictions. Here we will compute the probability of hitting a \r\n  letter-sized piece of paper and a half-letter-size piece of paper at increments \r\n  of 10cm from the target. I.e. pr_letter[20] is the probability of hitting a \r\n  letter-sized piece of paper from 2m away.\r\n  */\r\n  vector[200] pr_letter;\r\n  vector[200] pr_halfletter;\r\n  \r\n  \r\n  \r\n  for (ii in 1:200) {\r\n    \r\n    real dist = (ii+0.0)/10.0;\r\n    \r\n    real angle_x = atan(whLetter[1]/2/dist);\r\n    real angle_y = atan(whLetter[2]/2/dist);\r\n    \r\n    pr_letter[ii] = exp(log(student_t_cdf(angle_x, nu_x, 0.0, sigma_x)-student_t_cdf(-angle_x, nu_x, 0.0, sigma_x))\r\n                    +\r\n                    log(student_t_cdf(angle_y, nu_y, 0.0, sigma_y)-student_t_cdf(-angle_y, nu_y, 0.0, sigma_y))\r\n                    )\r\n                    ;\r\n                    \r\n    angle_x = atan(whLetter[1]/4/dist);\r\n    angle_y = atan(whLetter[2]/4/dist);\r\n    \r\n    pr_halfletter[ii] = exp(log(student_t_cdf(angle_x, nu_x, 0.0, sigma_x)-student_t_cdf(-angle_x, nu_x, 0.0, sigma_x))\r\n                    +\r\n                    log(student_t_cdf(angle_y, nu_y, 0.0, sigma_y)-student_t_cdf(-angle_y, nu_y, 0.0, sigma_y))\r\n                    )\r\n                    ;\r\n    \r\n    \r\n  }\r\n  \r\n  \r\n}\r\n\r\nAnd here is the model’s prediction for hitting the center of the target (i.e. Region 5) against the data:\r\n\r\n\r\nFit<-\"pooled.csv\" |>\r\n  read.csv() |>\r\n  mutate(\r\n    dist = parse_number(par)/10\r\n  ) |>\r\n  filter(!is.na(dist) & !grepl(\"prob\",par) & !grepl(\"lpr\",par)) |>\r\n  mutate(\r\n    Target = ifelse(grepl(\"pr_halfletter\",par),\"S\",\"L\")\r\n  )\r\n\r\nD<-\"../../data/NERF.csv\" |>\r\n  read.csv() |>\r\n  filter(Blaster!=\"\") |>\r\n  pivot_longer(\r\n    cols = X0:X9\r\n  )|>\r\n  group_by(Distance,Target,name) |>\r\n  summarize(\r\n    count=sum(value)\r\n  ) |>\r\n  group_by(Distance,Target) |>\r\n  summarize(\r\n    pr = sum(ifelse(name==\"X5\",count,0))/sum(count),\r\n    n = sum(count)\r\n  ) |>\r\n  mutate(\r\n    se = sqrt(pr*(1-pr)/n),\r\n    Distance = Distance*0.3048\r\n  )\r\n\r\n(\r\n  ggplot(Fit,aes(x=dist,fill=Target))\r\n  +geom_line(aes(y=mean,color=Target))\r\n  +geom_ribbon(aes(ymin=X25.,ymax=X75.),alpha=0.2)\r\n  +geom_ribbon(aes(ymin=X2.5.,ymax=X97.5.),alpha=0.2)\r\n  +geom_point(data=D,aes(x=Distance,y=pr,color=Target,fill=Target),size=5)\r\n  +geom_errorbar(data=D,aes(x=Distance,ymin=pr-1.96*se,ymax=pr+1.96*se,color=Target))\r\n  +theme_bw()\r\n  +xlab(\"distance to target (m)\")\r\n  +ylab(\"Probability of hittingh target (Region 5)\")\r\n  +coord_cartesian(xlim=c(0,10))\r\n)\r\n\r\n\r\n\r\nThat is, the red part of the plot shows the probability of hitting a letter-sized piece of paper. Shaded regions show 50% and 95% Bayesian credible regions.\r\nA 3-way fixed effects model\r\nNow, we will estimate a model allowing for each person, dart, and blaster to have their own effect on accuracy. Specifically, I will assume that the effects of each of these is additive (before doing the exponential transform, which makes sure parameters are positive). That is:\r\n\\[\r\n\\sigma_{x,p,d,b}=\\exp\\left(B^\\mathrm{person}_{p,\\sigma_x}+B^\\mathrm{dart}_{d,\\sigma_x}+B^\\mathrm{blaster}_{b,\\sigma_x}\\right)\r\n\\]\r\nwhere \\(p\\), \\(d\\), and \\(b\\) index the person, dart, and blaster, respectively. This is done similarly for \\(\\sigma_y\\).\r\nUnfortunately, I tried the Student’s-\\(t\\) specification for this and got a whole lot of divergent transitions, so what I will present here will be just for normal angle errors.\r\nAs these parameters are all going to be a mess to interpret, in the generated quantities block I generate predictions for the probability of hitting a letter-sized piece of paper from a distance of 5 meters. Here is the Stan program that does this:\r\n\r\n\r\ndata {\r\n  int<lower=0> N;\r\n  vector[N] count;\r\n  vector[N] angle_x_lb;\r\n  vector[N] angle_x_ub;\r\n  vector[N] angle_y_lb;\r\n  vector[N] angle_y_ub;\r\n  vector[N] miss;\r\n  \r\n  vector[2] whLetter;\r\n  \r\n  vector[2] prior_sigma;\r\n  \r\n  int ndarts;\r\n  int dartid[N];\r\n  \r\n  int nblasters;\r\n  int blasterid[N];\r\n  \r\n  int npeople;\r\n  int personid[N];\r\n\r\n}\r\n\r\n\r\nparameters {\r\n  \r\n  vector[ndarts] Bdart_sigma_x;\r\n  vector[nblasters] Bblaster_sigma_x;\r\n  vector[npeople] Bperson_sigma_x;\r\n  \r\n  \r\n  vector[ndarts] Bdart_sigma_y;\r\n  vector[nblasters] Bblaster_sigma_y;\r\n  vector[npeople] Bperson_sigma_y;\r\n  \r\n  \r\n  \r\n\r\n}\r\n\r\ntransformed parameters {\r\n  \r\n   \r\n  \r\n  \r\n}\r\n\r\n\r\nmodel {\r\n  \r\n  vector[N] sigma_x = exp(\r\n    Bdart_sigma_x[dartid]+Bblaster_sigma_x[blasterid]+Bperson_sigma_x[personid]\r\n  );\r\n \r\n  vector[N] sigma_y = exp(\r\n    Bdart_sigma_y[dartid]+Bblaster_sigma_y[blasterid]+Bperson_sigma_y[personid]\r\n  );\r\n \r\n  \r\n    vector[N] lpr;\r\n    for (ii in 1:N) {\r\n     lpr[ii] = log(normal_cdf(angle_x_ub[ii], 0.0, sigma_x[ii])-normal_cdf(angle_x_lb[ii], 0.0, sigma_x[ii]))\r\n                    +\r\n                    log(normal_cdf(angle_y_ub[ii],0.0, sigma_y[ii])-normal_cdf(angle_y_lb[ii],  0.0, sigma_y[ii]))\r\n                    ;\r\n    }                \r\n      lpr = lpr.*(1.0-miss)+log(1.0-exp(lpr)).*miss;\r\n    \r\n      target += count.*lpr;\r\n  \r\n  \r\n  target += normal_lpdf(Bdart_sigma_x | prior_sigma[1],prior_sigma[2]);\r\n  target += normal_lpdf(Bperson_sigma_x | prior_sigma[1],prior_sigma[2]);\r\n  target += normal_lpdf(Bblaster_sigma_x | prior_sigma[1],prior_sigma[2]);\r\n  target += normal_lpdf(Bdart_sigma_y | prior_sigma[1],prior_sigma[2]);\r\n  target += normal_lpdf(Bperson_sigma_y | prior_sigma[1],prior_sigma[2]);\r\n  target += normal_lpdf(Bblaster_sigma_y | prior_sigma[1],prior_sigma[2]);\r\n  \r\n}\r\n\r\ngenerated quantities {\r\n  \r\n  /* Predictions for a distance of 5m on the large target\r\n  */\r\n  \r\n  vector[nblasters] pr_blaster;\r\n  vector[ndarts] pr_dart;\r\n  vector[npeople] pr_person;\r\n  \r\n  {\r\n    real dist = 5.0;\r\n    \r\n    real angle_x = atan(whLetter[1]/2/dist);\r\n    real angle_y = atan(whLetter[2]/2/dist);\r\n    \r\n    real sigma_x;\r\n    real nu_x;\r\n    real sigma_y;\r\n    real nu_y;\r\n    \r\n    for (bb in 1:nblasters) {\r\n      \r\n      sigma_x = exp(Bblaster_sigma_x[bb]+mean(Bdart_sigma_x)+mean(Bperson_sigma_x));\r\n      sigma_y = exp(Bblaster_sigma_y[bb]+mean(Bdart_sigma_y)+mean(Bperson_sigma_y));\r\n      \r\n      pr_blaster[bb] = normal_cdf(angle_x,  0.0, sigma_x)-normal_cdf(-angle_x_lb, 0.0, sigma_x);\r\n      \r\n    }\r\n    \r\n    for (dd in 1:ndarts) {\r\n      \r\n      sigma_x = exp(Bdart_sigma_x[dd]+mean(Bblaster_sigma_x)+mean(Bperson_sigma_x));\r\n      sigma_y = exp(Bdart_sigma_y[dd]+mean(Bblaster_sigma_y)+mean(Bperson_sigma_y));\r\n      \r\n      pr_dart[dd] = normal_cdf(angle_x,  0.0, sigma_x)-normal_cdf(-angle_x_lb, 0.0, sigma_x);\r\n    }\r\n    \r\n    for (pp in 1:npeople) {\r\n      \r\n      sigma_x = exp(Bperson_sigma_x[pp]+mean(Bdart_sigma_x)+mean(Bblaster_sigma_x));\r\n      sigma_y = exp(Bperson_sigma_y[pp]+mean(Bdart_sigma_y)+mean(Bblaster_sigma_y));\r\n      \r\n      pr_person[pp] = normal_cdf(angle_x,  0.0, sigma_x)-normal_cdf(-angle_x_lb, 0.0, sigma_x);\r\n    }\r\n    \r\n  }\r\n  \r\n  \r\n}\r\n\r\nInstead of looking at the individual estimates of each B coefficient, I will focus on the model’s prediction for each group.\r\n\r\n\r\nlookup<-\"covariates_lookup.csv\" |>\r\n  read.csv() |>\r\n  select(-X)\r\n\r\nFit<-\"covariates.csv\" |>\r\n  read.csv() |>\r\n  select(-X) |>\r\n  filter(grepl(\"pr_\",par)) |>\r\n  mutate(\r\n    id = par |> parse_number(),\r\n    group = ifelse(grepl(\"person\",par),\"People\",\r\n                   ifelse(grepl(\"dart\",par),\"Darts\", \"Blasters\")\r\n                   )\r\n  ) |>\r\n  full_join(lookup,by=c(\"group\",\"id\")) |>\r\n  select(-par,-id) |>\r\n  group_by(group) |>\r\n  mutate(rank = rank(mean))\r\n  \r\n(\r\n  ggplot(Fit,aes(x=label))\r\n  +geom_point(aes(y=mean),color=\"red\")\r\n  +facet_wrap(~group,scales=\"free\")\r\n  +geom_errorbar(aes(ymin=X2.5.,ymax = X97.5.),alpha=0.3,color=\"red\")\r\n  +geom_errorbar(aes(ymin=X25.,ymax = X75.),alpha=0.5,color=\"red\")\r\n  +theme_bw()\r\n  +xlab(\"\")\r\n  +ylab(\"Probability of hitting a letter-sized\\n piece of paper from 5 meters\")\r\n)\r\n\r\n\r\n\r\nHere the dots show posterior means, and error bars show 50% and 95% credible regions. So there is quite a bit of posterior uncertainty left in all of these predictions. That said, the two stand-out improvements to accuracy are me (D) using the accustrike (AS) darts.\r\nWhat next?\r\nFrom collecting the data, it became apparent that some blasters had a much larger variance of the speed at which the dart left the blaster than others. As such, sometimes gravity had very little to do with the \\(y\\)-coordinate (when it was fast), and other times it had a lot to do with it. I would love to incorporate velocity variation into the model, but it would probably need a richer dataset.\r\nFurthermore, If I was to collect this kind of data again, I would vary the distance a bit more. According to the model, halving the distance and making the target half the size should result in the same probability of success. It would have been nicer to have some within-target variation in distance.\r\nShowing my working\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rstan)\r\noptions(mc.cores =parallel::detectCores())\r\nrstan_options(auto_write = TRUE)\r\n\r\nwhLetter <- c(0.2794,0.2159)\r\n\r\nregionlist<-data.frame(\r\n  region = c(0,1,2,3,4,5,6,7,8,9),\r\n  X = c(\"miss\",\"L\",\"M\",\"R\",\"L\",\"M\",\"R\",\"L\",\"M\",\"R\"),\r\n  Y = c(\"miss\",\"U\",\"U\",\"U\",\"C\",\"C\",\"C\",\"D\",\"D\",\"D\")\r\n)\r\n\r\nD<-\"data/NERF.csv\" |>\r\n  read.csv() |>\r\n  mutate(\r\n    Distance = 0.3048*Distance # convert into meters\r\n  ) |>\r\n  pivot_longer(\r\n    cols = X0:X9,\r\n    names_to = \"region\",\r\n    values_to = \"count\"\r\n  ) |>\r\n  filter(!is.na(Distance)) |>\r\n  mutate(\r\n    region = region |> parse_number()\r\n  ) |>\r\n  full_join(\r\n    regionlist,\r\n    by = \"region\"\r\n  ) |>\r\n  group_by(\r\n    Target,Blaster,Darts,Distance,Person,count,X,Y\r\n  ) |>\r\n  summarize(\r\n    count = sum(count)\r\n  ) |>\r\n  mutate(\r\n    wLetter = whLetter[1]*ifelse(Target==\"L\",1,0.5),\r\n    hLetter = whLetter[2]*ifelse(Target==\"L\",1,0.5),\r\n    theta.x = 2*atan(wLetter/2/Distance),\r\n    gamma.x = atan(3*wLetter/2/Distance)-theta.x/2,\r\n    theta.y = 2*atan(hLetter/2/Distance),\r\n    gamma.y = atan(3*hLetter/2/Distance)-theta.y/2,\r\n    \r\n    angle.x.lb = ifelse(X==\"M\",-theta.x/2,ifelse(X==\"miss\",-theta.x/2-gamma.x,theta.x/2)),\r\n    angle.x.ub = ifelse(X==\"M\",theta.x/2,ifelse(X==\"miss\",theta.x/2+gamma.x,theta.x/2+gamma.x)),\r\n    angle.y.lb = ifelse(Y==\"C\",-theta.y/2,ifelse(Y==\"miss\",-theta.y/2-gamma.y,theta.y/2)),\r\n    angle.y.ub = ifelse(Y==\"C\",theta.y/2,ifelse(Y==\"miss\",theta.y/2+gamma.y,theta.y/2+gamma.y)),\r\n    \r\n    miss = ifelse(X==\"miss\" & Y==\"miss\",1,0)\r\n  )\r\n\r\nfile<-\"_posts/2025-09-20-nerf-accuracy/pooled.csv\"\r\n\r\nif (!file.exists(file)) {\r\n\r\n  pooled<-\"_posts/2025-09-20-nerf-accuracy/pooled.stan\" |>\r\n    stan_model()\r\n  \r\n  d<-D |>\r\n    group_by(\r\n      angle.x.lb,angle.x.ub,angle.y.lb,angle.y.ub,miss\r\n    ) |>\r\n    summarize(\r\n      count = sum(count)\r\n    )\r\n  \r\n  \r\n  dStan<-list(\r\n    N = dim(d)[1],\r\n    count = d$count,\r\n    angle_x_lb = d$angle.x.lb,\r\n    angle_x_ub = d$angle.x.ub,\r\n    angle_y_lb = d$angle.y.lb,\r\n    angle_y_ub = d$angle.y.ub,\r\n    \r\n    miss = d$miss,\r\n    \r\n    whLetter = whLetter,\r\n    \r\n    prior_sigma = c(-1,1),\r\n    prior_nu = c(log(15),1),\r\n    \r\n    UseData=1\r\n  )\r\n  \r\n  Fit<-pooled |>\r\n    sampling(\r\n      data=dStan,\r\n      seed=42,\r\n      control = list(adapt_delta = 0.99),\r\n      iter = 2000\r\n    )\r\n  \r\n  summary(Fit)$summary |>\r\n    data.frame() |>\r\n    rownames_to_column(var = \"par\") |>\r\n    write.csv(file)\r\n}\r\n\r\n\r\nfile<-\"_posts/2025-09-20-nerf-accuracy/covariates.csv\"\r\n\r\nif (!file.exists(file) | TRUE) {\r\n  covariates<-\"_posts/2025-09-20-nerf-accuracy/covariates_normal.stan\" |>\r\n    stan_model()\r\n  \r\n  d<-D |>\r\n    group_by(\r\n      angle.x.lb,angle.x.ub,angle.y.lb,angle.y.ub,miss,Person,Darts,Blaster\r\n    ) |>\r\n    summarize(\r\n      count = sum(count)\r\n    ) |>\r\n    ungroup() |>\r\n    mutate(\r\n      dartid = Darts |> factor() |> as.numeric(),\r\n      blasterid = Blaster |> factor() |> as.numeric(),\r\n      personid = Person |> factor() |> as.numeric()\r\n    )\r\n  \r\n  \r\n  \r\n  lookup<-rbind(\r\n    d |> \r\n      group_by(Darts) |>\r\n      summarize(\r\n        id = mean(dartid)\r\n      ) |>\r\n      rename(\r\n        label=Darts\r\n      ) |>\r\n      mutate(\r\n        group = \"Darts\"\r\n      ),\r\n    d |> \r\n      group_by(Person) |>\r\n      summarize(\r\n        id = mean(personid)\r\n      ) |>\r\n      rename(\r\n        label=Person\r\n      ) |>\r\n      mutate(\r\n        group = \"People\"\r\n      ),\r\n    d |> \r\n      group_by(Blaster) |>\r\n      summarize(\r\n        id = mean(blasterid)\r\n      ) |>\r\n      rename(\r\n        label=Blaster\r\n      ) |>\r\n      mutate(\r\n        group = \"Blasters\"\r\n      )\r\n  )\r\n  \r\n  lookup |>\r\n    write.csv(\"_posts/2025-09-20-nerf-accuracy/covariates_lookup.csv\")\r\n  \r\n  dStan<-list(\r\n    N = dim(d)[1],\r\n    count = d$count,\r\n    angle_x_lb = d$angle.x.lb,\r\n    angle_x_ub = d$angle.x.ub,\r\n    angle_y_lb = d$angle.y.lb,\r\n    angle_y_ub = d$angle.y.ub,\r\n    \r\n    ndarts = d$dartid |> max(),\r\n    dartid = d$dartid,\r\n    npeople = d$personid |> max(),\r\n    personid = d$personid,\r\n    nblasters = d$blasterid |> max(),\r\n    blasterid = d$blasterid,\r\n    \r\n    \r\n    miss = d$miss,\r\n    \r\n    whLetter = whLetter,\r\n    \r\n    prior_sigma = c(-1,1)\r\n    \r\n    \r\n  )\r\n  \r\n  Fit<-covariates |>\r\n    sampling(\r\n      data=dStan,\r\n      seed=42,\r\n      control = list(adapt_delta = 0.99),\r\n      iter = 2000\r\n    )\r\n  \r\n  summary(Fit)$summary |>\r\n    data.frame() |>\r\n    rownames_to_column(var = \"par\") |>\r\n    write.csv(file)\r\n  \r\n   \r\n}\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2025-09-20-nerf-accuracy/nerf-accuracy_files/figure-html5/targetplot-1.png",
    "last_modified": "2025-09-21T16:43:11-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2025-09-20-nfl-field-goal-attempts/",
    "title": "NFL field goal attempts",
    "description": "Geometry and Stan",
    "author": [
      {
        "name": "James Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2025-09-20",
    "categories": [],
    "contents": "\r\nIt is currently AFL finals (playoffs) season, and I felt the urge to re-examine my set shots analysis I did a while back. Unfortunately, while the data exists for this and I imagine has vastly improved since my last look at it, one has to pay for it. So here I am on a Saturday afternoon, wanting to do some structural modelling of sports data as one does, and I’ve got my hands on some NFL field goal attempts data. This is going to look a lot like the attempt I did at this a few years ago, but now with some newer data and a hierarchical model.\r\nBy the way, if you’re interested in this kind of stuff, check out Andrew Gelman’s post on golf putting, which is what got me into this in the first place.\r\nModeling field goal attempts\r\nThe data\r\nI got the data from NFLSavant, which keeps play-by-play data from 2013-2024. I used the 2024 dataset (the most recent). I filtered the Description variable for FIELD GOAL, and classified most of it into the following categories:\r\nGood (i.e. successful, 3 points)\r\nMissed to the left or right. This included hitting the uprights.\r\nShort. This included hitting the crossbar\r\nBlocked\r\nThere were very few short and blocked instances. I decided that because of this there wasn’t enough information in the data to reasonably learn something about blocking and making the distance. Therefore I got rid of the short and blocked observations. What follows will therefore be just a model of getting the kicking angle right. I acknowledge that not modeling these probably makes the model less interesting and useful, but these are the data I have.\r\nThe geometry problem\r\nBecause of the very small number of misses due to not making the distance, what I am left with is needing to model getting the angle right, or right enough. Fortunately, there is a fairly straightforward relationship between the distance of the field goal attempt, and the angle a player needs to kick through. That is, this angle gets smaller for longer attempts. Here is what the problem looks like for a 30-yard attempt:\r\n\r\n\r\n\r\nThe distance between the uprights is \\(\\frac{18.5}{3}\\approx 6.17\\) yards, and so for an attempt \\(x\\) yards from the goal, the angle \\(\\theta\\) is:\r\n\\[\r\n\\begin{aligned}\r\n\\tan\\left(\\frac{\\theta}{2}\\right)&=\\frac{18.5}{3\\times 2\\times x}\\\\\r\n\\frac{\\theta}{2}&=\\tan^{-1}\\left(\\frac{18.5}{3\\times 2\\times x}\\right)\r\n\\end{aligned}\r\n\\]\r\nSo the angle of tolerance that a kicker faces looks like this:\r\n\r\n\r\n\r\nThe statistical problem\r\nNow we need to turn this geometry model into a statistical model. Suppose that the kick angle is distributed according to a normal distribution with standard deviation \\(\\sigma\\), and centered on a straight kick. Then the probability of a successful field goal attempt will be:\r\n\\[\r\n\\Pr(\\mathrm{good}\\mid \\mathrm{distance}=x)=\\Phi\\left(\\frac{\\theta(x)}{2\\sigma}\\right)-\\Phi\\left(-\\frac{\\theta(x)}{2\\sigma}\\right)=2\\Phi\\left(\\frac{\\theta(x)}{2\\sigma}\\right)-1\r\n\\]\r\nwhere \\(\\Phi(\\cdot)\\) is the standard normal cdf.\r\nThis is enough to write down a likelihood, but I wanted something a bit more flexible, so I went with a Student’s \\(t\\)-distribution instead.1 Therefore letting \\(F_t(x\\mid \\nu,\\sigma)\\) be the cdf of the \\(t\\) distribution centered on zero with \\(\\nu\\) degrees of freedom and spread parameter \\(\\sigma\\), I will use:\r\n\\[\r\n\\Pr(\\mathrm{good}\\mid \\mathrm{distance}=x)=2F_t\\left(\\frac{\\theta(x)}{2}\\mid \\nu,\\sigma\\right)-1\r\n\\]\r\nSo this will define my likelihood.\r\nAs I will be using Bayesian estimation for this, I also need to choose priors for \\(\\sigma\\) and \\(\\nu\\). For the degrees of freedom parameter \\(\\nu\\) I went with:\r\n\\[\r\n\\log\\nu\\sim N(\\log(15),1^2)\r\n\\]\r\nI chose this because the Student-\\(t\\) distribution starts to look a lot like a normal distribution at about \\(\\nu>30\\), so pinning down the median of the prior distribution to half this seemed to make sense.\r\nFor \\(\\sigma\\), I explored a few prior choices (holding the prior for \\(\\nu\\) constant) and their implications for the model’s predictions using the following Stan program, which I also use for estimation:\r\n\r\n\r\ndata {\r\n  int<lower=0> N;\r\n  vector[N] dist;\r\n  int good[N];\r\n  \r\n  vector[2] prior_sigma;\r\n  vector[2] prior_nu;\r\n  \r\n  int<lower=0,upper=1> UseData;\r\n  \r\n}\r\n\r\ntransformed data {\r\n  \r\n  \r\n  real goalpost_width = 18.5/3.0; // in yards\r\n  \r\n  vector[N] angle = atan(goalpost_width/(2.0*dist));\r\n  \r\n  \r\n}\r\n\r\nparameters {\r\n  real<lower=0> sigma;\r\n  real<lower=0> nu;\r\n  \r\n}\r\n\r\nmodel {\r\n  \r\n  if (UseData==1) {\r\n  \r\n    vector[N] pr_angle;\r\n  \r\n    for (ii in 1:N) {\r\n      pr_angle[ii] = 2.0*student_t_cdf(angle[ii], nu, 0.0, sigma)-1.0;\r\n    }\r\n  \r\n    target += bernoulli_lpmf(good | pr_angle);\r\n  }\r\n  \r\n  target += lognormal_lpdf(sigma | prior_sigma[1],prior_sigma[2]);\r\n  target += lognormal_lpdf(nu | prior_nu[1],prior_nu[2]);\r\n  \r\n  \r\n}\r\n\r\ngenerated quantities {\r\n  \r\n  // predicted probability\r\n  \r\n  vector[100] pr;\r\n  for (ii in 1:100) {\r\n    \r\n    pr[ii] = 2.0*student_t_cdf(atan(goalpost_width/(2.0*ii)), nu, 0.0, sigma)-1.0;\r\n    \r\n  }\r\n  \r\n}\r\n\r\nHere, I can turn off the likelihood contribution by setting UseData=0. Then, the predictions pr in the generated quantities block are draws from the prior. Eventually, I settled on:\r\n\\[\r\n\\log\\sigma\\sim N(-2,1)\r\n\\]\r\nwhich generates predictions like this:\r\n\r\n\r\nprior_predictive <-read.csv(\"individual_prior_predictive.csv\")\r\n\r\n(\r\n  ggplot(prior_predictive,aes(x=distance))\r\n  +geom_line(aes(y=X50.,linetype=\"median\"),linewidth=1)\r\n  +geom_line(aes(y=mean,linetype=\"mean\"),linewidth=1)\r\n  +geom_ribbon(aes(ymin = X25.,ymax=X75.),alpha=0.5)\r\n  +geom_ribbon(aes(ymin = X2.5.,ymax=X97.5.),alpha=0.5)\r\n  +theme_bw()\r\n  +xlab(\"Distance to goal (yards)\")\r\n  +ylab(\"Probability of success\")\r\n)\r\n\r\n\r\n\r\nHere the shaded regions show 50% and 95% Bayesian prior credible regions. This looked about right to me (without looking too much at the data), so I settled on these priors.\r\nEstimates\r\nPooled estimation\r\nFirst, I estimate a pooled model that estimates one \\(\\nu\\) and \\(\\sigma\\) for all of the data. Here are the estimates:\r\n\r\n\r\nFit<-\"individual.csv\" |>\r\n  read.csv() |>\r\n  mutate(\r\n    distance = par |> parse_number()\r\n  )\r\n\r\nFit |> \r\n  filter(is.na(distance)) |>\r\n  select(-distance,-X) |>\r\n  kbl(digits=3) |>\r\n  kable_classic(full_width=FALSE) |>\r\n  add_header_above(c(\"\",\"\",\"\",\"\",\"percentiles\"=5,\"\",\"\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\npercentiles\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\npar\r\n\r\n\r\nmean\r\n\r\n\r\nse_mean\r\n\r\n\r\nsd\r\n\r\n\r\nX2.5.\r\n\r\n\r\nX25.\r\n\r\n\r\nX50.\r\n\r\n\r\nX75.\r\n\r\n\r\nX97.5.\r\n\r\n\r\nn_eff\r\n\r\n\r\nRhat\r\n\r\n\r\nsigma\r\n\r\n\r\n0.048\r\n\r\n\r\n0.000\r\n\r\n\r\n0.002\r\n\r\n\r\n0.044\r\n\r\n\r\n0.047\r\n\r\n\r\n0.048\r\n\r\n\r\n0.050\r\n\r\n\r\n0.052\r\n\r\n\r\n1022.299\r\n\r\n\r\n1.003\r\n\r\n\r\nnu\r\n\r\n\r\n35.523\r\n\r\n\r\n0.946\r\n\r\n\r\n34.647\r\n\r\n\r\n7.539\r\n\r\n\r\n15.194\r\n\r\n\r\n24.698\r\n\r\n\r\n43.019\r\n\r\n\r\n124.870\r\n\r\n\r\n1342.192\r\n\r\n\r\n1.002\r\n\r\n\r\nlp__\r\n\r\n\r\n-389.363\r\n\r\n\r\n0.027\r\n\r\n\r\n0.940\r\n\r\n\r\n-391.875\r\n\r\n\r\n-389.731\r\n\r\n\r\n-389.096\r\n\r\n\r\n-388.692\r\n\r\n\r\n-388.425\r\n\r\n\r\n1253.353\r\n\r\n\r\n1.002\r\n\r\n\r\nBut they are quite hard to interpret, so let’s translate these into predictions:\r\n\r\n\r\nd<-\"cleaned_data.csv\" |>\r\n  read.csv()\r\nd_averages<-d |>\r\n  group_by(Distance) |>\r\n  summarize(\r\n    good = mean(Outcome==\"GOOD\"),\r\n    n=n()\r\n  )\r\n(\r\n  ggplot(Fit |> filter(!is.na(distance)),aes(x=distance))\r\n  +geom_point(data=d_averages,aes(x=Distance,y=good,size=n),alpha=0.5)\r\n  +geom_line(aes(y=mean),color=\"red\")\r\n  +geom_ribbon(aes(ymin = X25.,ymax=X75.),alpha=0.2,fill=\"red\")\r\n  +geom_ribbon(aes(ymin = X2.5.,ymax=X97.5.),alpha=0.2,fill=\"red\")\r\n  +geom_smooth(data=d,aes(x=Distance,y=1*(Outcome==\"GOOD\")),\r\n               method = \"glm\", \r\n               method.args = list(family = \"binomial\"))\r\n  +theme_bw()\r\n  +xlab(\"Distance to goal (yards)\")\r\n  +ylab(\"Probability of success\")\r\n  +ylim(c(0,1))\r\n)\r\n\r\n\r\n\r\nHere the red stuff is from the model, showing the mean (line) and 50% and 95% credible regions. The data are shown as black dots, and I put the logit line of best fit (blue) through there as well. While I don’t really believe that there is such a high probability of success for an attempt from (say) 75 yards, there isn’t any data out there to inform us about it. If there were, I would suspect that there would be a lot more misses due to not making the distance, and I would have been able to explicitly include them in my model.\r\nHierarchical estimation\r\nOf course, treating all observations as equal is not necessarily a good idea, and especially in this case where the data are generated by different kickers. I therefore also implemented a hierarchical model. Here I let every kicker’s \\(\\sigma\\) and \\(\\nu\\) be draws from a (transformed) multivariate normal distribution. In the generated quantities block, I calculate the prediction for each player’s probability of making a 40-yard field goal (as there were a lot of data around that distance). Here is the Stan program I wrote to estimate this model:\r\n\r\n\r\ndata {\r\n  int<lower=0> N;\r\n  int nkickers;\r\n  int id[N];\r\n  \r\n  \r\n  vector[N] dist;\r\n  int good[N];\r\n  \r\n  vector[2] prior_MU[2];\r\n  vector[2] prior_TAU;\r\n  real prior_Omega;\r\n  \r\n  \r\n  \r\n}\r\n\r\ntransformed data {\r\n  \r\n  \r\n  real goalpost_width = 18.5/3.0; // in yards\r\n  \r\n  vector[N] angle = atan(goalpost_width/(2.0*dist));\r\n  \r\n  \r\n}\r\n\r\nparameters {\r\n  vector[2] MU;\r\n  vector<lower=0.0>[2] TAU;\r\n  cholesky_factor_corr[2] L_Omega;\r\n  \r\n  matrix[2,nkickers] z;\r\n  \r\n}\r\n\r\ntransformed parameters {\r\n  \r\n  vector[nkickers] sigma;\r\n  vector[nkickers] nu;\r\n  \r\n  {\r\n    matrix[2,nkickers]  theta = rep_matrix(MU,nkickers)\r\n                    +diag_pre_multiply(TAU,L_Omega)*z;\r\n    sigma = exp(theta[1,]');\r\n    nu = exp(theta[2,]');\r\n  }\r\n  \r\n}\r\n\r\nmodel {\r\n  \r\n  \r\n  vector[N] pr_angle;\r\n  \r\n  for (ii in 1:N) {\r\n    pr_angle[ii] = 2.0*student_t_cdf(angle[ii], nu[id[ii]], 0.0, sigma[id[ii]])-1.0;\r\n  }\r\n  \r\n  target += bernoulli_lpmf(good | pr_angle);\r\n  \r\n  for (pp in 1:2) {\r\n    target += normal_lpdf(MU[pp] | prior_MU[pp][1],prior_MU[pp][2]);\r\n    target += cauchy_lpdf(TAU[pp] | 0.0, prior_TAU[pp]);\r\n  }\r\n  target += lkj_corr_cholesky_lpdf(L_Omega| prior_Omega);\r\n  target += std_normal_lpdf(to_vector(z));\r\n  \r\n  \r\n  \r\n}\r\n\r\ngenerated quantities {\r\n  \r\n  // predicted probability\r\n  \r\n  vector[nkickers] pr40;\r\n  for (ii in 1:nkickers) {\r\n    pr40[ii] = 2.0*student_t_cdf(atan(goalpost_width/(2.0*40.0)), nu[ii], 0.0, sigma[ii])-1.0;\r\n  }\r\n  \r\n}\r\n\r\nHere are the shrinkage estimates for the probability of making a 40-yard field goal for all 51 players in the dataset:\r\n\r\n\r\npr40<-\"hierarchical.csv\" |>\r\n  read.csv() |>\r\n  filter(grepl(\"pr40\",par)) |>\r\n  mutate(\r\n    id = par |> str_replace(\"pr40\",\"\") |> parse_number()\r\n  ) |>\r\n  full_join(\r\n    \"idlookup.csv\" |> read.csv(),\r\n    by=\"id\"\r\n  ) |>\r\n  mutate(\r\n    cumul = rank(mean)/n()-0.5/n()\r\n  )\r\n\r\n(\r\n  ggplot(pr40,aes(x=mean))\r\n  +stat_ecdf()\r\n  +geom_errorbar(aes(y=cumul,xmin=X25.,xmax=X75.),alpha=0.5)\r\n  +theme_bw()\r\n  +xlab(\"Probability of a successful 40-yard attempt\")\r\n  +ylab(\"Cumulative density of posterior mean\")\r\n)\r\n\r\n\r\n\r\nAnd here is a “top 10” list of field goal kickers based on the hierarchical model, ranked by their predicted probability of making a 40-yard filed goal. Here raw.mean lists each player’s fraction of successful attempts (from any distance). Note that this ranking is not simply a ranking of the raw means.\r\n\r\n\r\npr40 |> \r\n  arrange(-mean) |>\r\n  select(OffenseTeam,Kicker,raw.mean,mean,sd,n) |>\r\n  rename(attempts = n) |>\r\n  head(n=10) |>\r\n  kbl(digits=3) |>\r\n  kable_classic(full_width=FALSE) |> \r\n  add_header_above(c(\"\",\"\",\"\",\"estimates\"=2,\"\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nestimates\r\n\r\n\r\n\r\n\r\n\r\nOffenseTeam\r\n\r\n\r\nKicker\r\n\r\n\r\nraw.mean\r\n\r\n\r\nmean\r\n\r\n\r\nsd\r\n\r\n\r\nattempts\r\n\r\n\r\nPIT\r\n\r\n\r\n9-C.BOSWELL\r\n\r\n\r\n0.953\r\n\r\n\r\n0.929\r\n\r\n\r\n0.034\r\n\r\n\r\n43\r\n\r\n\r\nCHI\r\n\r\n\r\n8-C.SANTOS\r\n\r\n\r\n1.000\r\n\r\n\r\n0.927\r\n\r\n\r\n0.041\r\n\r\n\r\n21\r\n\r\n\r\nSEA\r\n\r\n\r\n5-J.MYERS\r\n\r\n\r\n0.963\r\n\r\n\r\n0.924\r\n\r\n\r\n0.038\r\n\r\n\r\n27\r\n\r\n\r\nWAS\r\n\r\n\r\n3-A.SEIBERT\r\n\r\n\r\n1.000\r\n\r\n\r\n0.922\r\n\r\n\r\n0.042\r\n\r\n\r\n27\r\n\r\n\r\nDEN\r\n\r\n\r\n3-W.LUTZ\r\n\r\n\r\n0.939\r\n\r\n\r\n0.914\r\n\r\n\r\n0.038\r\n\r\n\r\n33\r\n\r\n\r\nLAC\r\n\r\n\r\n11-C.DICKER\r\n\r\n\r\n0.932\r\n\r\n\r\n0.913\r\n\r\n\r\n0.035\r\n\r\n\r\n44\r\n\r\n\r\nTEN\r\n\r\n\r\n6-N.FOLK\r\n\r\n\r\n0.955\r\n\r\n\r\n0.911\r\n\r\n\r\n0.041\r\n\r\n\r\n22\r\n\r\n\r\nDAL\r\n\r\n\r\n17-B.AUBREY\r\n\r\n\r\n0.909\r\n\r\n\r\n0.910\r\n\r\n\r\n0.035\r\n\r\n\r\n44\r\n\r\n\r\nTB\r\n\r\n\r\n4-C.MCLAUGHLIN\r\n\r\n\r\n0.941\r\n\r\n\r\n0.910\r\n\r\n\r\n0.039\r\n\r\n\r\n34\r\n\r\n\r\nDET\r\n\r\n\r\n39-J.BATES\r\n\r\n\r\n0.931\r\n\r\n\r\n0.904\r\n\r\n\r\n0.041\r\n\r\n\r\n29\r\n\r\n\r\nIn case you’re interested, here is the bottom ten list as well:\r\n\r\n\r\npr40 |> \r\n  arrange(mean) |>\r\n  select(OffenseTeam,Kicker,raw.mean,mean,sd,n) |>\r\n  rename(attempts = n) |>\r\n  head(n=10) |>\r\n  kbl(digits=3) |>\r\n  kable_classic(full_width=FALSE) |> \r\n  add_header_above(c(\"\",\"\",\"\",\"estimates\"=2,\"\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nestimates\r\n\r\n\r\n\r\n\r\n\r\nOffenseTeam\r\n\r\n\r\nKicker\r\n\r\n\r\nraw.mean\r\n\r\n\r\nmean\r\n\r\n\r\nsd\r\n\r\n\r\nattempts\r\n\r\n\r\nNYJ\r\n\r\n\r\n9-G.ZUERLEIN\r\n\r\n\r\n0.643\r\n\r\n\r\n0.794\r\n\r\n\r\n0.070\r\n\r\n\r\n14\r\n\r\n\r\nCLE\r\n\r\n\r\n7-D.HOPKINS\r\n\r\n\r\n0.692\r\n\r\n\r\n0.798\r\n\r\n\r\n0.060\r\n\r\n\r\n26\r\n\r\n\r\nGB\r\n\r\n\r\n44-B.NARVESON\r\n\r\n\r\n0.706\r\n\r\n\r\n0.813\r\n\r\n\r\n0.061\r\n\r\n\r\n17\r\n\r\n\r\nCIN\r\n\r\n\r\n3-C.YORK\r\n\r\n\r\n0.818\r\n\r\n\r\n0.816\r\n\r\n\r\n0.068\r\n\r\n\r\n11\r\n\r\n\r\nWAS\r\n\r\n\r\n3-C.YORK\r\n\r\n\r\n0.000\r\n\r\n\r\n0.816\r\n\r\n\r\n0.068\r\n\r\n\r\n2\r\n\r\n\r\nSF\r\n\r\n\r\n4-J.MOODY\r\n\r\n\r\n0.750\r\n\r\n\r\n0.821\r\n\r\n\r\n0.054\r\n\r\n\r\n32\r\n\r\n\r\nCIN\r\n\r\n\r\n2-E.MCPHERSON\r\n\r\n\r\n0.727\r\n\r\n\r\n0.831\r\n\r\n\r\n0.056\r\n\r\n\r\n22\r\n\r\n\r\nATL\r\n\r\n\r\n6-Y.KOO\r\n\r\n\r\n0.781\r\n\r\n\r\n0.833\r\n\r\n\r\n0.050\r\n\r\n\r\n32\r\n\r\n\r\nBAL\r\n\r\n\r\n9-J.TUCKER\r\n\r\n\r\n0.750\r\n\r\n\r\n0.837\r\n\r\n\r\n0.051\r\n\r\n\r\n32\r\n\r\n\r\nNE\r\n\r\n\r\n13-J.SLYE\r\n\r\n\r\n0.839\r\n\r\n\r\n0.841\r\n\r\n\r\n0.050\r\n\r\n\r\n31\r\n\r\n\r\nShowing my working\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rstan)\r\nrstan_options(auto_write = TRUE)\r\noptions(mc.cores = parallel::detectCores())\r\n\r\ndFG<-\"https://nflsavant.com/pbp_data.php?year=2024\" |>\r\n  read.csv() |>\r\n  filter(grepl(\"FIELD GOAL\",Description)) |>\r\n  mutate(\r\n    Kicker = str_split_i(Description,\" \",2),\r\n    Outcome = ifelse(grepl(\"IS GOOD\",Description),\"GOOD\",ifelse(grepl(\"IS BLOCKED\",Description),\"BLOCKED\",str_split_i(Description,\", \",2))),\r\n    Distance = str_split_i(Description,\" \",3) |> parse_number()\r\n  ) |>\r\n  select(\r\n    GameId,GameDate,Description, OffenseTeam,Kicker,Outcome,YardLine,Distance\r\n  ) |>\r\n  filter(!is.na(Distance)) |>\r\n  mutate(\r\n    Outcome = ifelse(Outcome==\"HIT LEFT UPRIGHT\",\"WIDE LEFT\",Outcome),\r\n    Outcome = ifelse(Outcome==\"HIT RIGHT UPRIGHT\",\"WIDE RIGHT\",Outcome),\r\n    Outcome = ifelse(Outcome==\"HIT CROSSBAR\",\"SHORT\",Outcome)\r\n  ) |>\r\n  filter(Outcome==\"GOOD\" | Outcome==\"SHORT\" | Outcome==\"WIDE LEFT\" | Outcome==\"WIDE RIGHT\" | Outcome==\"BLOCKED\")\r\n\r\n\r\nd<-dFG |>\r\n  filter(Outcome!=\"SHORT\" & Outcome!=\"BLOCKED\") |>\r\n  mutate(\r\n    id = Kicker |> as.factor() |> as.integer()\r\n  )\r\n\r\nd |> \r\n  write.csv(\"_posts/2025-09-20-nfl-field-goal-attempts/cleaned_data.csv\")\r\n\r\nidlookup<-d |> \r\n  group_by(OffenseTeam,Kicker,id) |>\r\n  summarize(\r\n    n = n(),\r\n    `raw mean` = mean(Outcome==\"GOOD\"),\r\n    `av dist` = mean(Distance)\r\n  )\r\n\r\nidlookup |> \r\n  write.csv(\"_posts/2025-09-20-nfl-field-goal-attempts/idlookup.csv\")\r\n\r\n# prior predictive check\r\n\r\nindividual<-\"_posts/2025-09-20-nfl-field-goal-attempts/individual.stan\" |>\r\n  stan_model()\r\n\r\ndStan<-list(\r\n  N = dim(d)[1],\r\n  dist = d$Distance,\r\n  good = d$Outcome==\"GOOD\",\r\n  prior_sigma = c(-2,1),\r\n  prior_nu = c(log(15),1),\r\n  UseData=0\r\n)\r\n\r\nFit<-individual |>\r\n  sampling(\r\n    data=dStan,\r\n    seed=42\r\n  )\r\n\r\nprior_predictive<-summary(Fit)$summary |>\r\n  data.frame() |>\r\n  rownames_to_column(var = \"par\") |>\r\n  mutate(\r\n    distance = par |> parse_number()\r\n  ) |>\r\n  filter(!is.na(distance))\r\n\r\nprior_predictive |>\r\n  write.csv(\"_posts/2025-09-20-nfl-field-goal-attempts/individual_prior_predictive.csv\")\r\n\r\n(\r\n  ggplot(prior_predictive,aes(x=distance))\r\n  +geom_line(aes(y=X50.,linetype=\"median\"),linewidth=1)\r\n  +geom_line(aes(y=mean,linetype=\"mean\"),linewidth=1)\r\n  +geom_ribbon(aes(ymin = X25.,ymax=X75.),alpha=0.5)\r\n  +geom_ribbon(aes(ymin = X2.5.,ymax=X97.5.),alpha=0.5)\r\n  +theme_bw()\r\n  +xlab(\"Distance to goal\")\r\n  +ylab(\"Probability of success\")\r\n)\r\n\r\n#-------------------------------------------------------------------------------\r\n# Pooled model\r\n#-------------------------------------------------------------------------------\r\n\r\ndStan<-list(\r\n  N = dim(d)[1],\r\n  dist = d$Distance,\r\n  good = d$Outcome==\"GOOD\",\r\n  prior_sigma = c(-2,1),\r\n  prior_nu = c(log(15),1),\r\n  UseData=1\r\n)\r\n\r\nFit<-individual |>\r\n  sampling(\r\n    data=dStan,\r\n    seed=42\r\n  )\r\n\r\nsummary(Fit)$summary |> \r\n  data.frame() |>\r\n  rownames_to_column(var=\"par\") |>\r\n  write.csv(\"_posts/2025-09-20-nfl-field-goal-attempts/individual.csv\")\r\n\r\n\r\nfit<-summary(Fit)$summary |>\r\n  data.frame() |>\r\n  rownames_to_column(var = \"par\") |>\r\n  mutate(\r\n    distance = par |> parse_number()\r\n  ) |>\r\n  filter(!is.na(distance))\r\n\r\nd_averages<-d |>\r\n  group_by(Distance) |>\r\n  summarize(\r\n    good = mean(Outcome==\"GOOD\"),\r\n    n=n()\r\n  )\r\n\r\n\r\n(\r\n  ggplot(fit,aes(x=distance))\r\n  +geom_point(data=d_averages,aes(x=Distance,y=good,size=n),alpha=0.5)\r\n  +geom_line(aes(y=mean),linewidth=1,color=\"red\")\r\n  +geom_ribbon(aes(ymin = X25.,ymax=X75.),alpha=0.2,fill=\"red\")\r\n  +geom_ribbon(aes(ymin = X2.5.,ymax=X97.5.),alpha=0.2,fill=\"red\")\r\n  +geom_smooth(data=d,aes(x=Distance,y=1*(Outcome==\"GOOD\")),\r\n               method = \"glm\", \r\n               method.args = list(family = \"binomial\"))\r\n  +theme_bw()\r\n  +xlab(\"Distance to goal (yards)\")\r\n  +ylab(\"Probability of success\")\r\n  +ylim(c(0,1))\r\n)\r\n\r\n\r\nfile<-\"_posts/2025-09-20-nfl-field-goal-attempts/hierarchical.csv\"\r\n\r\nif (!file.exists(file)) {\r\n\r\n  hierarchical<-\"_posts/2025-09-20-nfl-field-goal-attempts/hierarchical.stan\" |>\r\n    stan_model()\r\n  \r\n  \r\n  dStan<-list(\r\n    N = dim(d)[1],\r\n    nkickers = d$id |> unique() |> length(),\r\n    id = d$id,\r\n    dist = d$Distance,\r\n    good = d$Outcome==\"GOOD\",\r\n    \r\n    prior_MU = list(\r\n      c(-2,1),\r\n      c(log(15),1)\r\n    ),\r\n    prior_TAU = c(1,1),\r\n    prior_Omega = 4\r\n  )\r\n  \r\n  Fit<-hierarchical |>\r\n    sampling(\r\n      data=dStan,\r\n      control = list(adapt_delta = 0.99),\r\n      seed=42,\r\n      par = \"z\",include=FALSE\r\n      )\r\n  \r\n  summary(Fit)$summary |>\r\n    data.frame() |>\r\n    rownames_to_column(var = \"par\") |>\r\n    write.csv(file)\r\n  \r\n\r\n  \r\n}\r\n\r\n\r\n\r\nThis turned out to be a really important form of flexibility when I was modeling set shots in Australian football, where both the geometry is more complicated and the data are richer, but it didn’t really produce anything interesting here↩︎\r\n",
    "preview": "posts/2025-09-20-nfl-field-goal-attempts/nfl-field-goal-attempts_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2025-09-20T16:15:55-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-20-monitoring-soil-moisture-and-sneding-it-to-google-sheets/",
    "title": "Monitoring soil moisture and sending it to Google Sheets",
    "description": "Because my green thumb is useless, but I can solder and program an Arduino",
    "author": [
      {
        "name": "James Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2024-12-23",
    "categories": [],
    "contents": "\r\n.\r\nMy better half and favorite co-author very much likes her houseplants. It is not unusual for her to come home and unapologetically present a new acquisition which she will manage to find space for. Now, while I fully support this hobby, I am not particularly good at it, and so my involvement in it usually only rises to the level of “hold this so I i can put some soil around the roots”. That is, until we started talking about the important variables to monitor in order to keep a plant happy.\r\nOne of my hobbies is electronics. I love to tinker with Arduinos and the like. So once the important variables (moisture, temperature, light, and humidity) were identified, it was not long before we got the idea to build a thingy to keep track of these. It also doesn’t hurt that my better half is also an econometrician, and so is fully on board with having more data to work with.\r\nSo I set out to build this thingy with the following goals:\r\nLog soil moisture, temperature, light, and humidity\r\nPush this information to something easily accessible, like an online spreadsheet or web page, and\r\nHave a couple of LEDs to communicate some of its operations.\r\nI did this in a few stages. Here they are!\r\nVersion 1: Arduino nano on a breadboad\r\nThe first attempt was a proof-of-concept with just one one component. For this, I used the Monk Makes Plant Monitor, which measures temperature, humidity, and soil moisture. Three out of four isn’t too bad for the first try! This was a relatively easy hook-up:\r\n\r\n\r\nArduino\r\n\r\n\r\nPlant monitor\r\n\r\n\r\n3.3V\r\n\r\n\r\n3V\r\n\r\n\r\nGND\r\n\r\n\r\nGND\r\n\r\n\r\nTX\r\n\r\n\r\nRX, through a 1k resistor\r\n\r\n\r\nRX\r\n\r\n\r\nTX\r\n\r\n\r\nAnd then I just ran the example code supplied in the above link. Everything worked, moisture went up when I dropped it in a cup of water, humidity and temperature went up when I breathed on it, all good!\r\nThe problem with this thingy was that it had to be plugged into my laptop for us to be able to read the data. As this was not a good long-term solution, I asked my better half if she would rather have the readings shown on a screen, or posted to somewhere on the web. She opted for the latter, so on the next iteration, I focused on connecting it to the internet.\r\nVersion 2: Connecting to the interwebs\r\nAs much as I love Arduinos, the cheap ones don’t connect to the internet. This meant for the second thingy, I would have to choose another microcontroller to use. Fortunately, I had a whole lot of WeMos D1 mini clones that would do this perfectly. I also had acquired some other capacitave soil sensors that didn’t require serial communication, and were substantially cheaper. This would mean I’d need to also have temperature and humidity sensors, so I went with the AHT10, which does both over I2C with 3.3V logic. This last part is important becuase the D1 mini pins work with 3.3V, not the 5V of Arduino.\r\nOn top of these, I added a momentary switch for recording events (e.g. so we could press it when we watered the plant), and two LEDs for communicating different things about the status of the thingy.\r\nFor the software, I found this tutorial on how to send data from an ESP8266 module (of which the D1 mini is one) to a Google sheet. This seemed like a good solution, as we could both have access to the data easily.\r\nVersion 3: Getting the most out of one analog pin\r\nFrom here, the only thing left to do was add in a light sensor. It is fairly easy to set up a circuit that responds to light: use a light-dependent resistor in series with a (regular) resistor, and read off the voltage between them. However here the D1 mini poses a problem: it only has one analog pin, and I was already using it to measure moisture. One possible solution was to use a multiplexer integrated circuit, like a 4051. I had some of these lying around, but it seemed like overkill: it could handle eight analog inputs (I only needed two), and needs three digital inputs to manage, and that would require more wiring. Fortunately, this tutorial was exactly what I needed: basically, instead of powering the two sensors with the 3.3V pin so that they were always on, you use separate digital pins for power. This means that you can always power down a sensor, so its voltage goes to zero. Diodes then ensure that the signals only go to the analog pin, and don’t get drained into the other sensor.\r\nHere is what I ended up with for the hardware:\r\n.\r\nThat is, in order to take the two analog readings:\r\nSet pin D0 to HIGH and pin D5 to LOW. This fires up the moisture probe and turns off the light meter. Diode D1 blocks any current from draining into the light meter.\r\nRead the voltage at the analog pin (A0). This is the moisture reading.\r\nSet D0 to LOW and D5 to HIGH. This turns off the moisture meter and turns on the light meter. Diode D2 blocks any current from draining into the moisture reading.\r\nRead the voltage at the analog pin (A0). This is the light reading.\r\nSome other features of this design that I have are:\r\nLED1 and LED2 light up when the light and moisture meters are being read, respectively. I put these in so that I could tell if the wiring and software were working properly.\r\nLED3 is there to communicate other things. At the moment I am just blinking it when the microcontroller is sending data to the Google sheet.\r\nI use the momentary switch (S1) for logging events. These are things like watering the plant, moving a humidifier in or out of a room, etc.\r\nFinally, here is the Arduino code I used to program the microcontroller.\r\nI set the Thingy to log data every ten minutes, which is probably overkill for this project, but I got impatient.\r\nA look at the data\r\nHere is what the data look like:\r\n.\r\nSome things to note about these:\r\nWe first watered the plant being monitored by PlantMonitor0001 on Dec 18th. You can see this in the plot as a large drop in the moisture reading. The voltage the moisture sensor gives is negatively related to moisture, reading about 800 when dry and in the air, and below 400 when submersed in a glass of water.\r\nWe watered PlantMonitor0002 on Dec 22nd. I think the slight increase in moisture reading before it dropped out after watering was due to shifting the plant around.\r\nThe vertical dashed lines show when the button was pushed on the monitor. We use this to mark significant events for the plants. For example the red vertical lines corresponding to large increases in humidity mark when my better half misted some water into the room.\r\nPlantMonitor0001 does not have a light sensor, so we don’t get any light data from that\r\nPlantMonitor0002 is in the same room as our house’s thermostat, but PlantMonitor0001 is not. This really highlighted to us how much of a temperature swing we can get in this room.\r\nThe drops in light measurements each day from about 800 to about 650 are when the grow lights turn off, and the drop down after then is when we turn our living room lights off.\r\nNext steps\r\nProbably the thing I’d like to fix the most out of these is a connection problem I am having with out WiFi. Roughly every 24 hours, the thingies get stuck in their transmit stage (I know this because of the status of the LEDs on the boards) and just stop sending data. Ironing this out would mean I would not need to go and reset them every day or so. Resetting is not hard to do, but it does mean that I need to be watching the data come in somewhat regularly so that I notice it quickly. Ideally these things would be set-and-forget, and we would only have to look at the data when we wanted to.\r\nOnce we have collected enough data, I would like to re-program the boards to send notification reminders for watering.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2025-09-20T08:31:33-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-04-26-the-predictor-corrector-algorithm-for-computing-qre/",
    "title": "Computing logit Quantal Response Equilibrium",
    "description": "Implementing a predictor-corrector algorithm in *R*",
    "author": [
      {
        "name": "James Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2023-05-03",
    "categories": [],
    "contents": "\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(kableExtra)\r\nlibrary(gganimate)\r\n\r\n\r\n\r\n\r\n\r\nTed Turocy and I recently wrote a working paper on the ins and outs of computing and estimating Quantal Response Equilibrium (QRE). One of the issues we discuss in this paper is how computing QRE is computationally difficult, but that there are some things we can do to make the problem easier for our computer to solve. This post will walk you through a predictor-corrector algorithm we discuss in the paper to compute logit QRE (LQRE).\r\nConsider, as an example the following plot of data from Selten and Chmura’s (2008) experiment1, which has participants play one of twelve generalized matching pennies games. Each dot shows the average play for a group of eight for the last 100 (of 200) rounds of the experiment. In red, I have labeled the unique mixed strategy Nash equilibrium of the game with an “N”. “R” marks the “centroid”, which corresponds to uniform randomization.\r\n\r\n\r\n\r\nHere we can see that while the data seem to match Nash equilibrium much better than uniform randomization (good), there still seems to be a lot of behavior that is not well organized by Nash equilibrium. Quantal Response equilibrium is a generalization of Nash equilibrium, and so the hope is that QRE will organize the data better than does Nash equilibrium.\r\nWhat is Quantal Response Equilibrium?\r\nQRE is an extension of Nash equilibrium. Specifically, QRE replaces the best response with a probabilistic best response. That is, instead of players responding perfectly to equilibrium mixed strategies, they do so with some noise. As with Nash equilibrium, players have correct beliefs about the strategies of their opponents.\r\nThe quantal response part\r\nSo what does a probabilistic best response look like? The usual assumption made about this is that actions that yield greater utility are chosen with greater probability. For example, suppose that a player is has two actions to choose between, \\(a\\) and \\(b\\). A popular specification of probabilistic best response is the logit specification, which assumes that the probability of choosing action \\(a\\) is:\r\n\\[\r\n\\begin{aligned}\r\n\\Pr(a\\mid u(a),u(b))&=\\frac{\\exp(\\lambda u(a))}{\\exp(\\lambda u(a))+\\exp(\\lambda u(b))}\\\\&\r\n=\\frac{1}{1+\\exp(-\\lambda(u(a)-u(b)))}\r\n\\end{aligned}\r\n\\]\r\nwhere \\(u(a)\\) is the utility from taking action \\(a\\), and \\(u(b)\\) is the utility of choosing action \\(b\\). \\(\\lambda>0\\) is the choice precision parameter. Here is a plot of this function for several values of \\(\\lambda\\):\r\n\r\n\r\n\r\nNote that the probability of choosing \\(a\\) is increasing as \\(a\\) yields more utility than \\(b\\). Also, as \\(\\lambda\\) increases, the function becomes more sensitive to changes in payoffs. Taken to its limit, as \\(\\lambda\\to\\infty\\), this function in fact becomes the best response function (approximated in the figure by the \\(\\lambda=3000\\) curve). At the other end of \\(\\lambda\\)’s\r\nrange, \\(\\lambda=0\\), the probabilistic best response function becomes flat, meaning that the player will randomize uniformly between actions \\(a\\) and \\(b\\).\r\nBut why are we doing this? Firstly, probabilistic best response is behaviorally plausible, or at least more plausible than best response. Our participants are human after all, and so assuming that they are going to perfectly best respond to payoffs and their opponents’ strategies is less plausible than assuming a probabilistic response. Therefore the predictions of QRE might (and in fact often do) deviate from Nash equilibrium in the direction that we see them do in data from experiments.\r\nSecondly, the probabilistic best response assumption enables us to use likelihood-based estimation techniques like maximum likelihood and Bayesian estimation. Specifically, in Nash equilibrium, players will never choose an action that is not in their best response correspondence. Therefore if our human participants make just one decision that goes against this prediction, the likelihood function of our data will be zero everywhere, and we will not be able to estimate our model. Furthermore, and probably more alarmingly, best response participants will never choose a strictly dominated action, and so we should worry about the zero likelihood problem here too.\r\nFinally, another interpretation of the probabilistic best response function is that there is a component of utility that is known to the participant, but not observed by us, the econometrician. That is, we can motivate probabilistic best response by assuming that players’ actual utility difference is:\r\n\\[\r\nu(a)-u(b)+\\epsilon\r\n\\]\r\nand that they best respond based on this quantity. Probabilistic best response therefore allows us to acknowledge that we may not have written down the utility function correctly.\r\nThe equilibrium part\r\nJust like Nash equilibrium is when every player’s strategy is a best response to all other players’ strategies, in QRE every player’s strategy is a probabilistic best response to all other players’ strategies. Formally, if we let \\(p\\) be the stacked vector of players’ mixed strategies, then using the logit specification, (logit) QRE is a solution to:\r\n\\[\r\np_{i,a}=\\frac{\\exp(\\lambda u_{i,a}(p))}{\\sum_{b\\in A_i}\\exp(\\lambda u_{i,b}(p))}\\quad \\text{for all } i\\in N,\\ a\\in A_i \\quad (\\star)\r\n\\]\r\nSo equilibrium is a fixed point in the above system of equations. The hard thing about computing QRE is the the mixed strategy profile \\(p\\) appears both on the left- and the right-hand side of this equation, so we generally can’t find an analytic solution to this.\r\nFor Game 1 of the Selten & Chmura (2008) experiment:\r\n\r\n\r\n\r\n\r\nL\r\n\r\n\r\nR\r\n\r\n\r\nU\r\n\r\n\r\n10, 8\r\n\r\n\r\n0, 18\r\n\r\n\r\nD\r\n\r\n\r\n9, 9\r\n\r\n\r\n8, 8\r\n\r\n\r\nthis is how we would graphically find QRE for a specific value of \\(\\lambda\\):\r\n\r\n\r\nurow<-rbind(c(10,0),c(9,8))\r\nucol<-rbind(c(8,9),c(18,8))\r\n\r\nlambda<-2\r\n\r\nd<-expand.grid(p = seq(0,1,length=101),player = c(\"row\",\"column\")) |>\r\n  rowwise() |>\r\n  mutate(\r\n    PBR = ifelse(player==\"row\",\r\n                 1/(1+exp(-lambda*(c(1,-1)%*%urow%*%c(p,1-p))))\r\n                 ,\r\n                 1/(1+exp(-lambda*(c(1,-1)%*%ucol%*%c(p,1-p))))\r\n                 )\r\n  ) |>\r\n  ungroup()\r\n\r\n(\r\n  ggplot()\r\n  +geom_path(data = d |> filter(player==\"row\"),\r\n             aes(x=PBR,y=p,color=\"row\"))\r\n  +geom_path(data = d |> filter(player==\"column\"),\r\n             aes(x=p,y=PBR,color=\"column\"))\r\n  +coord_fixed()\r\n  +theme_bw()\r\n  +xlab(\"Up\")+ylab(\"Left\")\r\n)\r\n\r\n\r\n\r\nSo the QRE is the intersection of the probabilistic responses, just like Nash equilibrium is the intersection of best responses. While this is a great way to visualize QRE for \\(2\\times2\\) games, it is not such a good way of solving for QRE. In practice, we will be better off using numerical continuation methods to solve QRE. Specifically, we will first define a system of equations that characterizes QRE. We will then use a predictor-corrector algorithm to solve the system.\r\nSetting up a system of equations\r\nFirst, we will set up a system of equations describing LQRE. These will include two kinds of constraints:\r\nEquilibrium constraints specify a condition that must be true in equilibrium. Specifically, we will need a constraint that looks like equation \\((\\star)\\) for every action for very player (except for one base action per player).\r\nAdding-up constraints ensure that probabilities add to one. That is, there might exist solutions to \\((\\star)\\) that are not proper probabilities.\r\nFurthermore, because the problem can be unstable if we attempt to solve it in probability levels, we will solve it in log probabilities. This has the added bonus of taking care of the other constraint on probabilities: that they must be greater than zero.\r\nTo begin with, let’s start with the equilibrium constraints. If we take the log of both sides of \\((\\star)\\), we get:\r\n\\[\r\n\\begin{aligned}\r\n\\log p_{i,a}&=\\lambda u_{i,a}(p)-\\log\\left(\\sum_{b\\in A_i}\\exp\\left(\\lambda u_{i,b}(p)\\right)\\right)\r\n\\end{aligned}\r\n\\]\r\nWe can then subtract the expression for \\(\\log p_{i,0}\\), a baseline action (log) probability for player \\(i\\), from both sides of the equation to get:\r\n\\[\r\n\\begin{aligned}\r\n\\log p_{i,a}-\\log p_{i,0}&=\\lambda u_{i,a}(p)-\\lambda u_{i,0}(p)-\\log\\left(\\sum_{b\\in A_i}\\exp\\left(\\lambda u_{i,b}(p)\\right)\\right)+\\log\\left(\\sum_{b\\in A_i}\\exp\\left(\\lambda u_{i,b}(p)\\right)\\right)\\\\\r\n&=\\lambda\\left[u_{i,a}(p)-u_{i,0}(p)\\right]\r\n\\end{aligned}\r\n\\]\r\nThat is, we can express a difference in log probabilities as a difference in utilities, amplified by \\(\\lambda\\).\r\nLetting \\(l_{i,a}=\\log p_{i,a}\\), we can therefore write the equilibrium constraints as:\r\n\\[\r\n\\begin{aligned}\r\nH_{i,a}(l,\\lambda)&=l_{i,a}-l_{i,0}-\\lambda\\left[u_{i,a}(\\exp(l))-u_{i,0}(\\exp(l))\\right]\\\\\r\n\\end{aligned}\r\n\\]\r\nThat is, for \\((l,\\lambda)\\) to be an LQRE, it must be that \\(H_{i,a}(l,\\lambda)=0\\) for all \\(i\\) and \\(a\\).\r\nFinally, we need one constraint per player to ensure that their mixed strategy probabilities add up to one:\r\n\\[\r\n\\begin{aligned}\r\nH_i(l,\\lambda)&=1-\\sum_{a=1}^{|A_i|}\\exp(l_{i,a})\r\n\\end{aligned}\r\n\\]\r\nFor Game 1 of Selten & Chmura (2008), shown in the table above, this means we would have:\r\n\\[\r\n\\begin{aligned}\r\nH(l,\\lambda)&=\\begin{pmatrix}\r\nl_U-l_D-\\lambda\\left[\\exp(l_L)-8\\exp(l_R)\\right]\\\\\r\nl_L-l_R-\\lambda\\left[-10\\exp(l_U)+\\exp(l_D)\\right]\\\\\r\n1-\\exp(l_U)-\\exp(l_D)\\\\\r\n1-\\exp(l_L)-\\exp(l_R)\r\n\\end{pmatrix}\r\n\\end{aligned}\r\n\\]\r\nSometimes in the later working, I find it easier to write this in this form:\r\n\\[\r\n\\begin{aligned}\r\nH(l,\\lambda)&=Al+B(\\lambda)\\exp(l)+c\\\\\r\n\\text{where: } A&=\\begin{bmatrix}\r\n1 & -1 & 0 & 0\\\\\r\n0 & 0 & 1& -1\\\\\r\n0 & 0& 0& 0\\\\\r\n0 & 0 & 0& 0\r\n\\end{bmatrix}\\\\\r\nB(\\lambda)&=\\begin{bmatrix}0& 0& -\\lambda & 8\\lambda\\\\\r\n10\\lambda &-\\lambda &0&0\\\\\r\n-1 & -1&  0 & 0\\\\\r\n0 & 0 & -1 & -1\r\n\\end{bmatrix}\\\\\r\nc&=\\begin{pmatrix}0 \\\\ 0 \\\\ 1\\\\1\\end{pmatrix}\r\n\\end{aligned}\r\n\\]\r\nwhich cuts down on some of the notational burden when we have to take derivatives of \\(H\\). For two-player games, these matrices are usually very simple (but not necessarily small), and should contain a lot of zeros and ones.\r\nPredicting and correcting\r\nThe goal for computing QRE is to trace out the solutions to \\(H(l,\\lambda)=0\\), starting at a known solution \\((l^0,\\lambda^0)\\). Fortunately for us, we know that uniform randomization with \\(\\lambda = 0\\), i.e. \\((\\bar l,0)\\), is a QRE. So this is a good first place to start.2 The predictor-corrector algorithm makes two kinds of step. These are:\r\nPredictor steps. Here, we make an approximation of how \\(l\\) will change as we change \\(\\lambda\\) from \\(\\lambda^t\\) to \\(\\lambda^{t+1}\\). We do this by making a linear approximation of \\(l^{t+1}\\) as a function of \\(\\lambda\\).\r\nCorrector steps. Since the predictor step is only an approximation, we need to correct any errors (because \\(H(l^{t+1},\\lambda^{t+1})\\neq 0\\)) associated with this approximation. Holding \\(\\lambda^{t+1}\\) constant, we search for a solution to \\(H(l,\\lambda^{t+1})=0\\). This is done through Newton’s approximation.\r\nThe predictor step\r\nThe predictor step uses a linear approximation of \\(H\\) to determine how the equilibrium log-probabilities \\(l\\) change due to a change in \\(\\lambda\\). That is, the total derivative of \\(H\\) can be written as:\r\n\\[\r\n\\begin{aligned}\r\n0&=\\frac{\\partial H(l,\\lambda)}{\\partial l^\\top}\\mathrm dl+\\frac{\\partial H(l,\\lambda)}{\\partial \\lambda}\\mathrm d\\lambda\r\n\\end{aligned}\r\n\\]\r\nTherefore if we know that \\((l^t,\\lambda^t)\\) is a QRE, then going along the path in the direction \\(\\Delta\\lambda^t\\) tells us that \\((l^t+\\Delta l^t,\\lambda^t+\\Delta\\lambda^t)\\) will be approximately a QRE, where:\r\n\\[\r\n\\Delta l^t =-\\left[\\frac{\\partial H(l^t,\\lambda^t)}{\\partial l^\\top}\\right]^{-1}\\frac{\\partial H(l^t,\\lambda^t)}{\\partial \\lambda}\\Delta\\lambda^t\r\n\\]\r\n\\(l^t+\\Delta l^t\\) then becomes our initial guess for the (log) mixed strategy profile at \\(\\lambda^{t+1}=\\lambda^t+\\Delta\\lambda^t\\)\r\nThe corrector step\r\nBut the above is only an approximation of QRE for \\(\\lambda^{t+1}\\). This is because the system is nonlinear, and we have made a linear approximation. We therefore need to correct our guess for \\(l^{t+1}\\). This is for two reasons:\r\nThe QRE conditions will only be approximately true, and\r\nThe sum-to-one conditions will only be approximately be true.\r\nFortunately, both of these conditions are captured in \\(H(l,\\lambda)\\), and we can approximate this, just as a function of \\(l\\), as follows:\r\n\\[\r\n\\begin{aligned}\r\nH(l^{t+1},\\lambda)-H(l^{t},\\lambda)&\\approx \\frac{\\partial H(l,\\lambda)}{\\partial \\lambda^\\top}(l^{t+1}-l^t)\r\n\\end{aligned}\r\n\\]\r\nThis is a linear approximation of \\(H(l^{t},\\lambda)\\).\r\nwe want to find a solution to \\(H(l^{t+1},\\lambda)=0\\), so:\r\n\\[\r\nl^{t+1}=l^t-\\left[\\frac{\\partial H(l,\\lambda)}{\\partial \\lambda^\\top}\\right]^{-1}H(l^t,\\lambda)\r\n\\]\r\nAnd so, as long as the corrector step has not flung us too far off course, we can iterate on this until we are sufficiently close to a zero for \\(H(l^t,\\lambda)\\).\r\nAn example game\r\nLet’s use Selten & Chmura’s (2008) Game 1 as an example again, and implement the predictor-corrector algorithm to trace out the LQRE. For this, we will need \\(H\\) and its first derivitives. Using the matrix representation:\r\n\\[\r\nH(l,\\lambda)=Al+B(\\lambda)\\exp(l)+c\r\n\\]\r\nthese derivatives are:\r\n\\[\r\n\\begin{aligned}\r\n\\frac{\\partial H(l,\\lambda)}{\\partial l^\\top} &=A+B(\\lambda)\\mathrm{diag}(\\exp(l))\\\\\r\n\\frac{\\partial H(l,\\lambda)}{\\partial \\lambda}&=B_\\lambda\\exp(l)\r\n\\end{aligned}\r\n\\]\r\nwhere \\(\\mathrm{diag}(x)\\) takes a vector \\(x\\) and turns it into a diagonal matrix.\r\nAnd here they are coded up:\r\n\r\n\r\nA<-rbind(c(1,-1,0,0),\r\n         c(0,0,1,-1),\r\n         c(0,0,0,0),\r\n         c(0,0,0,0)\r\n         )\r\nB<-function(lambda) {\r\n  rbind(c(0,0,-lambda,8*lambda),\r\n        c(10*lambda,-lambda,0,0),\r\n        c(-1,-1,0,0),\r\n        c(0,0,-1,-1)\r\n  )\r\n}\r\n\r\nB_lambda<-rbind(c(0,0,-1,8),\r\n        c(10,-1,0,0),\r\n        c(0,0,0,0),\r\n        c(0,0,0,0)\r\n  )\r\n\r\nc<-c(0,0,1,1)\r\n\r\nH<-function(l,lambda) {\r\n  A%*%l+B(lambda)%*%exp(l)+c\r\n}\r\n\r\nHl<-function(l,lambda) {\r\n  A+B(lambda)%*%diag(exp(l) |> as.vector())\r\n}\r\n\r\nHlambda<-function(l,lambda) {\r\n  B_lambda%*%exp(l)\r\n}\r\n\r\n\r\nNow let’s use these to trace out the QRE for the game. Here I am not exactly implementing my most efficient code to do this. Specifically, I am storing all of the information about intermediate steps so we can look at them later. Also, I am not optimizing the algorithm for step length. Instead, I am just specifying a grid for \\(\\lambda\\). I will get to fixing this in another post.\r\n\r\n\r\n# grid to compute QRE over\r\nLAMBDA<-1/(1-seq(0.1,0.9,length=9))-1\r\n\r\n# initial conditions\r\nl<-rep(log(0.5),4)\r\nlambda<-0\r\n\r\n# Tolerance for the corrector step\r\ntol<-1e-6\r\n\r\nQRE<-tibble(\r\n  U = l[1],\r\n  D = l[2],\r\n  L = l[3],\r\n  R = l[4],\r\n  lambda = lambda,\r\n  step=\"initial\",\r\n  err=0\r\n)\r\n\r\nfor (ll in LAMBDA) {\r\n  \r\n  #  predictor step\r\n  dl <- solve(Hl(l,lambda),Hlambda(l,lambda))*(ll-lambda)\r\n  \r\n  l<-l+dl\r\n  lambda<-ll\r\n  \r\n  QRE<-rbind(\r\n    QRE,\r\n    tibble(\r\n      U = l[1],\r\n      D = l[2],\r\n      L = l[3],\r\n      R = l[4],\r\n      lambda = lambda,\r\n      step = \"predictor\",\r\n      err = max(abs(H(l,lambda)))\r\n    )\r\n  )\r\n  \r\n  \r\n  # corrector steps\r\n  err<-Inf\r\n  \r\n  while (err>tol) {\r\n    \r\n    Hfun<-H(l,lambda)\r\n    \r\n    err<-max(abs(Hfun))\r\n    \r\n    l<-l-solve(Hl(l,lambda),Hfun)\r\n    \r\n    QRE<-rbind(\r\n    QRE,\r\n    tibble(\r\n      U = l[1],\r\n      D = l[2],\r\n      L = l[3],\r\n      R = l[4],\r\n      lambda = lambda,\r\n      step = \"corrector\",\r\n      err = max(abs(H(l,lambda)))\r\n    )\r\n    )\r\n    \r\n  }\r\n  \r\n  QRE<-rbind(\r\n    QRE,\r\n    tibble(\r\n      U = l[1],\r\n      D = l[2],\r\n      L = l[3],\r\n      R = l[4],\r\n      lambda = lambda,\r\n      step = \"final\",\r\n      err = max(abs(H(l,lambda)))\r\n    )\r\n    )\r\n  \r\n  \r\n}\r\n\r\n\r\nLet’s have a look at what we’ve got, and I will overlay the data from this game as well:\r\n\r\n\r\nQRE<-QRE |>\r\n  mutate(Up = exp(U),Down = exp(D),Left = exp(L),Right = exp(R),\r\n         t = 1:n())\r\n\r\n(\r\n  ggplot(QRE |> filter(step==\"final\"),aes(x=Up,y=Left))\r\n  +geom_point(data=SC2008_100 |> filter(game==1),aes(x=Up,y=Left))\r\n  +geom_path()\r\n  +theme_bw()\r\n  +coord_fixed()\r\n)\r\n\r\n\r\n\r\nNice! The data are actually matching the QRE arc really well!\r\nLet’s have a look at what the predictor-corrector algorithm is doing:\r\n\r\n\r\n(\r\n  ggplot() \r\n  +geom_point(data=QRE,aes(x=Up,y=Left,color=step))\r\n  +geom_path(data=QRE |> filter(step==\"final\") |> select(Up,Left),aes(x=Up,y=Left),linetype=\"dashed\")\r\n  +transition_time(t)\r\n  +theme_bw()\r\n  +coord_fixed()\r\n)\r\n\r\n\r\n\r\nSo here we can see that, at least in the probability space, the algorithm is all over the place, but finds the QRE arc in the end. But this is a 5-dimensional problem (four probabilities plus \\(\\lambda\\)), which makes it hard to really see everything that’s going on a 2-dimensional page. Perhaps the best way I could come up with to see this was to show every step of the algorithm on one axis, and how the probabilities add up on the other:\r\n\r\n\r\n(\r\n  ggplot(QRE,aes(x=t))\r\n  +geom_path(aes(y=Up+Down,color=\"Up+Down\"))\r\n  +geom_path(aes(y=Left+Right,color = \"Left+Right\"))\r\n  +theme_bw()\r\n  +geom_vline(data=QRE|> filter(step==\"final\"),aes(xintercept = t),linetype=\"dashed\")\r\n  +ylab(\"Sum of probabilities\")\r\n)\r\n\r\n\r\n\r\nWhere the vertical lines show where the while loop in the corrector algorithm stopped, which is the final value for that \\(\\lambda\\). It looks like the predictor step is systematically predicting “probabilities” that add up to something greater than one, and this is (part of) what the corrector steps are correcting for.\r\n\r\nSelten, Reinhard, and Thorsten Chmura. “Stationary concepts for experimental 2x2-games.” American Economic Review 98, no. 3 (2008): 938-966.↩︎\r\nAlternatively, we could start at (approximatly) a Nash equilibrium \\((l^\\text{Nash},\\lambda\\to\\infty)\\) and work our way “backwards”. This adds some additional complexity because our computer won’t be able to handle infinity, but once we are away from Nash equilibrium and heading in the right direction, the corrector step should be able to find the solution fairly easily.↩︎\r\n",
    "preview": "posts/2023-04-26-the-predictor-corrector-algorithm-for-computing-qre/the-predictor-corrector-algorithm-for-computing-qre_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-09-20T08:31:31-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-08-a-test-for-omitted-variables/",
    "title": "RESET: A test for omitted variables?",
    "description": "Yes, but nope",
    "author": [
      {
        "name": "James Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2022-09-08",
    "categories": [],
    "contents": "\r\nYesterday a colleague asked me about the Ramsey RESET\r\ntest. A student had asked her about whether it could be used to test\r\nfor omitted variables. Specifically, whether this test could be used to\r\ndetermine whether an explanatory variable is endogenous.\r\nThe short answer to this question is a resounding no, but it\r\ngot us thinking about the disctinction between two kinds of omitted\r\nvariables:\r\nThe kind we usually worry about, which are the ones that never show\r\nup in our dataset. These are what we usually think about when one of our\r\nexplanatory variables is endogeneous.\r\nA transform of one of the variables we include. For example if we\r\nonly include \\(x\\) linearly in our\r\nspecification, then we are leaving out \\(x^2\\), \\(x^3\\), \\(\\log\r\nx\\), \\(x^{-1}\\), \\(\\exp(x)\\), and so on.\r\nUnsurprisingly, the test is designed to pick up (2), and (1) is\r\nsomething we can never test for (outside of experiments). But I can\r\nunderstand why this student saw parallels between RESET and endogeneity.\r\nLet’s take a deeper dive\r\nWhat is RESET?\r\nRESET asks if the residuals from your regression have significant\r\nexplanatory power for your dependent variable. Specifically, if you\r\nestimate the equation using OLS:\r\n\\[\r\ny_i=\\beta_0+\\beta_1x_i+\\epsilon_i\r\n\\]\r\nthen if the model is correctly specified, then estimating this again\r\nby adding a polynomial of residuals from the first model on the\r\nright-hand side should not get you any better explanatory power. That\r\nis, after estimating the model above, we have residuals \\(\\{\\hat y_i\\}_{i=1}^N\\), and we include\r\npowers of these on the right-hand-side:\r\n\\[\r\ny_i=\\alpha_0+\\alpha_1x_i+\\gamma_1\\hat y^2_i+\\gamma_2\\hat\r\ny_i^3+\\ldots+\\epsilon_i\r\n\\] then all of the \\(\\gamma\\)\r\nterms should be zero. This is very easy to implement in R using\r\nthe resettest function in the lmtest\r\npackage.\r\nA simulation\r\nwith misspecified functional form\r\nWhen does this perform well (or as expected)? That happens when what\r\nwe’re missing from our specification is a more complex relationship\r\nbetween \\(x\\) and \\(y\\). Here is a data-generating process that\r\ndoes this:\r\n\\[\r\n\\begin{aligned}\r\ny_i&=x_i+0.5x_i^2+\\epsilon_i\\\\\r\nx_i, \\epsilon_i&\\sim iid N(0,1)\r\n\\end{aligned}\r\n\\] That is, suppose we estimate a model including \\(x\\) linearly on the right-hand side. What\r\nkind of relationship do we get. Let’s simulate one sample to find\r\nout:\r\n\r\n\r\nlibrary(tidyverse)\r\nset.seed(42)\r\nN<-1000\r\n\r\nx<-rnorm(N)\r\ny<-x+0.5*x^2+rnorm(N)\r\n\r\nd<-tibble(y,x)\r\n\r\n(\r\n  ggplot(data=d,aes(x=x,y=y))+theme_bw()\r\n  +geom_point(alpha=0.5)\r\n  +geom_smooth(method=\"lm\",formula=\"y~x\")\r\n  +geom_abline(slope=1,intercept=0,linetype=\"dashed\")\r\n)\r\n\r\n\r\n\r\nactually this model doesn’t do a bad job of picking up that \\(\\beta_1=1\\) because the squared term is\r\nsymmetric around zero, and so is the distribution of \\(x\\). That is why we get a line of best fit\r\nclose to parallel with the \\(45^\\circ\\)\r\nline (dashed line).\r\nBut is is clearly a bad fit. Not to worry, we can work out that we\r\nhave this kind of omitted variable using RESET:\r\n\r\n\r\nlibrary(lmtest)\r\n\r\nresettest(data=d,y ~ x , power=2, type=\"regressor\") %>% print()\r\n\r\n\r\n    RESET test\r\n\r\ndata:  y ~ x\r\nRESET = 572.75, df1 = 1, df2 = 997, p-value < 2.2e-16\r\n\r\nSo we reject the null, which is good in this case because the null is\r\nfalse: the simple bivariate model is mis-specified because we have not\r\nincluded the \\(x^2\\) term.\r\nA simulation with\r\nendogeneity\r\nOK, but what about the other kind of omitted variable? To\r\nme, at least, when somebody says “omitted variable” I typically think of\r\na variable that is not included in the model, and that\r\npractically can’t be included in the model because it is not in\r\nour data.\r\nFor this part, let’s use the classic endogeneity example of\r\nestimating the causal effect of education on wages. Here ability is the\r\nconfounding, omitted, variable.\r\nLet’s assume that the true model is:\r\n\\[\r\n\\begin{aligned}\r\n\\mathrm{wage}_i&=\\beta_0+\\beta_1\\mathrm{education}_i+\\beta_2\\mathrm{ability}_i+\\epsilon_i\\\\\r\n\\mathrm{education}_i&=\\gamma_0+\\gamma_1\\mathrm{ability}_i+\\eta_i\r\n\\end{aligned}\r\n\\]\r\nWe will set \\(\\beta_0=\\gamma_0=0\\),\r\n\\(\\beta_1=\\beta_2=\\gamma_1=1\\) for\r\nsimplicity, let \\(\\epsilon_i,\\eta_i\\sim iid\r\nN(0,1)\\), and education and ability are also standard normal.\r\nImportantly, the effect of education on wage is one. So any estimator\r\nthat on average gets us something other than one is biased.\r\nHere is some code that draws from this distribution:\r\n\r\n\r\nset.seed(42)\r\n\r\nN<-1000 # sample size\r\nability<-rnorm(N)\r\neducation<-ability+rnorm(N)\r\nwage = education+ability+rnorm(N)\r\n\r\nd<-tibble(wage,education)\r\n\r\n(\r\n  ggplot(data=d,aes(x=education,y=wage))\r\n  +geom_point(alpha=0.5)\r\n  +geom_smooth(method=\"lm\",formula=\"y~x\")\r\n  +geom_abline(slope=1,intercept = 0,linetype=\"dashed\")\r\n  +theme_bw()\r\n)\r\n\r\n\r\n\r\nSince the blue line is the line of best fit (OLS), and the dashed\r\nline has a slope of one, if we are unbiased estimating \\(\\beta_1\\), these lines should lie on top of\r\neach other on average (or at least have the same slope).\r\nThey don’t. Why? Because the error term, call it \\(u_i\\), in the estimated equation includes\r\nability:\r\n\\[\r\n\\mathrm{wage}_i=\\beta_0+\\beta_1\\mathrm{education}_i+\\underbrace{\\beta_2\\mathrm{ability}_i+\\epsilon_i}_{u_i}\r\n\\] and:\r\n\\[\r\n\\begin{aligned}\r\n\\mathrm{cov}(\\mathrm{education}_i,u_i)\r\n&=\\mathrm{cov}(\\mathrm{education}_i,\\beta_2\\mathrm{ability}_i+\\epsilon_i)\\\\\r\n&=E\\left[(\\mathrm{education}_i-E(\\mathrm{education}_i))(\\mathrm{ability}_i-E(\\mathrm{ability}_i))\\right]\\\\\r\n&=E\\left[\\mathrm{education}_i\\mathrm{ability}_i\\right]\\quad\r\n\\text{(both have zero mean)}\\\\\r\n&=E[(\\gamma_0+\\gamma_1\\mathrm{ability}_i+\\eta_i)\\mathrm{ability}_i]\\\\\r\n&=\\gamma_1E[\\mathrm{ability}_i^2]\\\\\r\n&\\neq 0\r\n\\end{aligned}\r\n\\]\r\nHere is how we can estimate the blue line:\r\n\r\n\r\nlibrary(stargazer)\r\n\r\nreg1<-lm(data=d,formula=wage~education)\r\n\r\nstargazer(reg1,type=\"html\")\r\n\r\n\r\n\r\n\r\n\r\n\r\nDependent variable:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nwage\r\n\r\n\r\n\r\n\r\neducation\r\n\r\n\r\n1.488***\r\n\r\n\r\n\r\n(0.028)\r\n\r\n\r\n\r\n\r\n\r\n\r\nConstant\r\n\r\n\r\n-0.014\r\n\r\n\r\n\r\n\r\n(0.039)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nObservations\r\n\r\n\r\n1,000\r\n\r\n\r\nR2\r\n\r\n0.741\r\n\r\n\r\nAdjusted R2\r\n\r\n0.741\r\n\r\n\r\nResidual Std. Error\r\n\r\n\r\n1.244 (df = 998)\r\n\r\n\r\nF Statistic\r\n\r\n\r\n2,856.708*** (df = 1; 998)\r\n\r\n\r\n\r\n\r\nNote:\r\n\r\n\r\np<0.1; p<0.05;\r\np<0.01\r\n\r\n\r\nThe RESET test includes powers of \\(\\hat\r\ny_i\\) on the RHS of the regression. If these have significant\r\nexplanatory power for \\(y_i\\), we\r\nreject the null of no omitted variable.\r\n\r\n\r\nresettest(data=d,wage ~ education , power=2, type=\"regressor\") %>% print()\r\n\r\n\r\n    RESET test\r\n\r\ndata:  wage ~ education\r\nRESET = 0.0054627, df1 = 1, df2 = 997, p-value = 0.9411\r\n\r\nso we would fail to reject the null of no omitted variables, even\r\nthough the null is, by construction, false. Since the residuals are just\r\na linear function of education, the polynomial of residuals is just a\r\npolynomial of education, which understandably has no explanatory power\r\nfor wages, because I haven’t included any polynomial terms in the\r\ndata-generating process.\r\nBut was I just lucky in this particular draw of data? Let’s do this\r\n1,000 times just to be sure. To do this, I write a function that\r\ngenerates a sample, then outputs the slope coefficient (\\(\\hat\\beta_1\\)) of the regression, as well\r\nas the \\(p\\)-value of the test:\r\n\r\n\r\nsimStep<-function(N) {\r\n  ability<-rnorm(N)\r\n  education<-ability+rnorm(N)\r\n  wage = education+ability+rnorm(N)\r\n  d<-tibble(wage,education)\r\n  reg<-lm(data=d,formula=wage~education)$coefficients[\"education\"]\r\n  test<-resettest(data=d,wage ~ education , power=2, type=\"regressor\")$p.value\r\n  \r\n  c(reg,test)\r\n}\r\n\r\ncoef<-c()\r\npval<-c()\r\n\r\nfor (ss in 1:1000) {\r\n  sim<-simStep(1000)\r\n  coef<-c(coef,sim[1])\r\n  pval<-c(pval,sim[2])\r\n}\r\n\r\nsimData<-tibble(coef,pval)\r\n\r\n\r\nso we would reject the null this fraction of the time:\r\n\r\n\r\nmean(simData$pval<0.05)\r\n\r\n[1] 0.043\r\n\r\neven though the null is false:\r\n\r\n\r\n(\r\n  ggplot(data=simData,aes(x=coef))\r\n  +geom_histogram()\r\n  +theme_bw()\r\n  +geom_vline(xintercept=1)\r\n)\r\n\r\n\r\n\r\nNote that rejecting the null 4.3% of the time when the null is\r\nfalse is a bad thing. We want to be rejecting the null\r\nhere, so we recognize that we have that we have an omitted variable. In\r\nfact, we are rejecting the null about as often as we would were it true.\r\nAs a Bayesian, my interpretation of this is that the test gives me\r\nabsolutely no information about whether I have an omitted\r\nvariable.\r\nThe trouble is that RESET isn’t trying to spot this kind of omitted\r\nvariables. While it might be good for working out if you’ve got your\r\nfunction form right (i.e. should I include some polynomial terms?), it\r\nis not intended to alert you to an endogenous variable.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-08-a-test-for-omitted-variables/a-test-for-omitted-variables_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2025-09-20T08:31:28-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-14-estimating-qre/",
    "title": "Estimating Quantal Response Equilibrium in an attacker-defender game",
    "description": "A Bayesian implementation of some econometrics in Holt, Sahu, and Smith (2022)",
    "author": [
      {
        "name": "James Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2022-07-21",
    "categories": [],
    "contents": "\nOne of the things I really like about quantal response equilibrium (QRE) is that it is right in the intersection of game theory, behavioral economics, and statistics. The game theory part should be pretty clear, because QRE is an equilibrium concept for games. It draws from behavioral economics because one can interpret the imprecision of players’ decisions as bounded rationality, which is empirically very relevant. The statistical part comes in because it permits the use of a likelihood function in estimating a model’s parameters, where Nash equilibrium sometimes does not (for example, when a player has a strictly dominant strategy).\nAs I have begun writing in this area, and was fortunate enough to attend the 25+ years of Quantal Response Equilibrium Conference at Caltech in March this year, I thought I’d write a quick post about what QRE is, and show you how I am estimating its parameters using Bayesian techniques. Of course, if you are looking for my full working paper on how to do this, you should click here, but what follows is a more introductory treatment of the process. I have also had many questions from people about how to get started with this, so I suppose I can direct them here from now on.\nSometimes Nash equilibrium makes weird predictions\nA recent paper in Southern Economic Journal highlights one of the interesting features of QRE, in that it often predicts the direction in which play by human subjects will depart from Nash equilibrium. Here is the full citation with a link to the published paper:\n\nHolt, Charles A., Ricky Sahu, and Angela M. Smith. “An experimental analysis of risk effects in attacker‐defender games.” Southern Economic Journal (2022).\n\nhenceforth HSS.\nHere are the payoff tables of HSS’ three treatments:\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\npayoffs<-list()\npayoffs$low<-list()\npayoffs$low$Urow<-rbind(c(1,1),c(4,-2))\npayoffs$low$Ucol<-rbind(c(1,0),c(1,6)) %>% t()\npayoffs$medium<-list()\npayoffs$medium$Urow<-rbind(c(1,1),c(4,-2))\npayoffs$medium$Ucol<-rbind(c(1,-2),c(1,4)) %>% t()\npayoffs$high<-list()\npayoffs$high$Urow<-rbind(c(1,1),c(4,-2))\npayoffs$high$Ucol<-rbind(c(1,-4),c(1,2)) %>% t()\n\n#payoffs %>% print()\n\nfor (ii in 1:length(payoffs)) {\n  pp<-payoffs[[ii]]\n  tab<-paste(pp$Urow,\",\", pp$Ucol %>% t()) %>% matrix(nrow=2)\n  rownames(tab)<-c(\"High alert\",\"Low alert\")\n  colnames(tab)<-c(\"Wait\",\"Attack\")\n  print(paste(names(payoffs)[ii],\"alert cost ------------------\"))\n  knitr::kable(tab) %>% print()\n  print(\"\")\n}\n\n[1] \"low alert cost ------------------\"\n\n\n|           |Wait  |Attack |\n|:----------|:-----|:------|\n|High alert |1 , 1 |1 , 0  |\n|Low alert  |4 , 1 |-2 , 6 |\n[1] \"\"\n[1] \"medium alert cost ------------------\"\n\n\n|           |Wait  |Attack |\n|:----------|:-----|:------|\n|High alert |1 , 1 |1 , -2 |\n|Low alert  |4 , 1 |-2 , 4 |\n[1] \"\"\n[1] \"high alert cost ------------------\"\n\n\n|           |Wait  |Attack |\n|:----------|:-----|:------|\n|High alert |1 , 1 |1 , -4 |\n|Low alert  |4 , 1 |-2 , 2 |\n[1] \"\"\n\nNote that the row player’s payoff do not change over these three treatments. That is, from the row player’s perspective, their part of the payoff matrix is always:\n\n\npayoffs$low$Urow\n\n     [,1] [,2]\n[1,]    1    1\n[2,]    4   -2\n\nBecause this game has a unique Nash equilibrium in mixed strategies, Nash equilibrium predicts that the column player’s probability of choosing “Attack” should not change between treatments.\nWhat?\nI just showed you that the row player’s payoffs remained the same, but that tells us something about the column player? To see why, let’s calculate the mixed strategy Nash equilibrium strategy for the column player, which must set the probability of attacking, call this \\(p_A\\), such that the row player is indifferent between being on low and high alert:\n\\[\n\\begin{aligned}\nU_\\text{High}&=1p_A+1(1-p_A)=1\\\\\nU_\\text{Low}&=-2p_A+4(1-p_A)=4-6p_A\\\\\n\\text{indifference}\\implies 1&=4-6p_A\\\\\n6p_A&=3\\\\\np_A=\\frac{1}{2}\n\\end{aligned}\n\\]\nNote here that we did not need to look at the column player’s payoffs in making this calculation, but we have calculated the column player’s mixed strategy! This is a property of all mixed strategy Nash predictions: if you only change the payoffs of one player, that player should not adjust their strategy,\nSo what happens in reality? The data for this experiment are are available at the group level in the paper (thanks, authors!), so one can just copy and paste them into a spreadsheet and then import them right into R:\n\n\nd<-read_excel(\"HSSdata.xls\",sheet=\"Final15\")\ncolnames(d)<-c(\"Session\",\"Treatment\",\"PairID\",\"attack\",\"alert\")\nd<-(\n  d\n  %>% mutate(cost = ifelse(Treatment==\"Low\",0,\n                           ifelse(Treatment==\"Medium\",2,4)))\n)\nd %>% sample_n(10) %>% knitr::kable()\n\nSession\nTreatment\nPairID\nattack\nalert\ncost\nter7\nMedium\n2022-01-07\n0.27\n0.47\n2\nter8\nLow\n2022-05-11\n0.13\n0.67\n0\nter7\nMedium\n2022-04-10\n0.27\n0.33\n2\nadnf10\nHigh\n2022-04-10\n0.07\n0.07\n4\nter9\nHigh\n2022-05-11\n0.13\n0.00\n4\nter9\nHigh\n2022-04-10\n0.33\n0.13\n4\nter7\nMedium\n2022-06-12\n0.40\n0.53\n2\nadnf9\nMedium\n2022-02-08\n0.20\n0.67\n2\nadnf9\nMedium\n2022-01-07\n0.20\n0.93\n2\nter8\nLow\n2022-06-12\n0.33\n0.80\n0\n\nHere I (and HSS) am focusing on the final 15 rounds of play, as earlier rounds may be less likely to have equilibrium play.\nBut when we look at the probability that each player attacks, broken down by treatment, we see that higher attack costs result in lower attack probabilities.\n\n\n(\n  ggplot(d,aes(attack,color=Treatment))\n  +stat_ecdf()+theme_bw()\n  +geom_vline(xintercept=1/2,linetype=\"dashed\")\n)\n\n\n\nHere I am plotting the empirical cumulative density function of the fraction of times each column player chose “Attack”. Looking at this, the red line (high attack cost) is at least as far to the left of the blue line (medium attack cost), which itself is almost always to the left of the green line (low attack cost). That is, attack probabilities are increasing as attack costs go down. This is quite intuitive to anyone who has taken a principles course: if you raise the price (cost of attacking), you are less likely to buy the good (attack), but it is not predicted by Nash equilibrium.\nIntroducing quantal response equilibrium (QRE)\nFortunately, Nash equilibrium is not the only tool we have to work with. While it seems intuitively plausible that people should attack less when the attack cost goes up, Nash predicts that the opponent (row player) will adjust their strategy to keep the column player indifferent.\nQRE replaces the Nash equilibrium’s best response function with a “probabilistic best response”, or “noisy best response” function. The idea is that while players respond to their payoffs, they don’t do it perfectly. In particular, we will assume that players are more likely to take actions that yield higher expected utility, but they don’t necessarily always choose the action with the highest expected utility. This is behaviorally plausible because boundedly rational players may correctly evaluate their expected utilities on average, but they make mistakes in their calculations.\nA popular parameterization of a probabilistic best response function is logit choice, also called softmax:\n\\[\np(a\\mid u)=\\frac{\\exp(\\lambda u_a)}{\\sum_j\\exp(\\lambda u_j)}\n\\]\nwhere \\(ui\\) is a vector of expected payoffs, \\(\\u_a\\) is the expected payoff of choosing action \\(a\\), and \\(\\lambda\\geq 0\\) is the choice precision parameter. When there are only two actions to choose from, this becomes:\n\\[\n\\begin{aligned}\np(a\\mid u_a,u_b)&=\\frac{\\exp(\\lambda u_a)}{\\exp(\\lambda u_a)+\\exp(\\lambda u_b)}\\\\\n&=\\left(1+\\exp(-\\lambda(u_a-u_b))\\right)^{-1}\n\\end{aligned}\n\\]\nand it is also useful to express this in logit form:\n\\[\n\\log p_a-\\log p_b=\\lambda(u_a-u_b)\n\\] which is true for an arbitrary number of actions. This is useful because it shows how players respond to utility differences: when \\(u_a>u_b\\) players will be more likely to choose \\(a\\) than \\(b\\). Increasing this difference will make it even more likely, and so will increasing the choice precision parameter \\(\\lambda\\). Furthermore, this model nests two important responses to payoffs that are empirically interesting:\nWhen \\(\\lambda = 0\\), players uniformly randomize over their action space\nWhen \\(\\lambda\\to\\infty\\), the probabilistic best response approaches a deterministic best response. For me, this is best seen in a graph:\n\n\nlambda<-c(0,1,10,100)\nDU<-seq(-1,1,length=101)\npltThis<-tibble()\nfor (ll in lambda) {\n  tmp<-(tibble(DU = DU,lambda = ll)\n           %>% mutate(pA = 1/(1+exp(-lambda*DU)))\n  )\n  pltThis<-rbind(pltThis,tmp)\n}\n\n(\n  ggplot(pltThis,aes(x=DU,y=pA,color=as.factor(lambda)))\n  +geom_path()\n  +xlab(\"Ua - Ub\")+ylab(\"p(a)\")+theme_bw()\n)\n\n\n\nSo what we have up to this point is a replacement for the best response function. What we need to do now is to tie it in with the fixed point condition of Nash equilibrium, where we impose that the row player’s best response to the column player’s strategy is the row player’s strategy. Likewise for the column player.\nFor quantal response equilibrium, this means that the row player’s probabilistic best response to the column player’s strategy is their own mixed strategy, so using the logit specification:\n\\[\n\\begin{aligned}\n\\log p_H-\\log p_L&=\\lambda\\left[u_H-u_L\\right]\\\\\n&=\\lambda\\left[1-(4-6p_A)\\right]\\\\\n&=\\lambda(6p_A-3)\n\\end{aligned}\n\\]\nand noting that \\(p_L=1-p_L\\), this becomes:\n\\[\n\\log(p_H)-\\log(1-p_H)=\\lambda(6p_A-3)\n\\] Similarly for the column player, we need that:\n\\[\n\\begin{aligned}\n\\log(p_A)-\\log(1-p_A)&=\\lambda\\left[u_A-u_W\\right]\\\\\n&=\\lambda\\left[6(1-p_H)-c-1\\right]\n\\end{aligned}\n\\] where \\(c\\in\\{0,2,4\\}\\) is the relevant attack cost for that treatment.\nPutting these together, we have two equations in two unknowns, which written as zeros becomes: \\[\n\\begin{aligned}\n0&=\\log (p_H)-\\log (1-p_H)-\\lambda(2-5p_A)\\\\\n0&=\\log(p_A)-\\log(1-p_A)-\\lambda\\left[6(1-p_H)-c-1\\right]\n\\end{aligned}\n\\] which is not solvable by hand, but fortunately a computer can do it for us. While solving the above directly will probably work just fine for this application, it is usually better for stability reasons to solve the problem in logs or logits directly, then transform back into probabilities:\n\n\n# Objective function: minimize the difference between the RHS of the above equations and zero\nObjFun<-function(lp,c,lambda) {\n  p<-1/(1+exp(-lp)) # convert logit(p) to probabilities\n  \n  x<-c(\n    lp[1]-lambda*(6*p[2]-3),\n    lp[2]-lambda*(6*(1-p[1])-c-1)\n  )\n  \n  sum(x^2)\n}\n\n# Equilibrium calculation: minimize the objective function given c and lambda\nQRE<-function(c,lambda) {\n  par<-optim(c(0,0),function(p) ObjFun(p,c,lambda))$par\n  1/(1+exp(-par))\n}\n\nQRE(0,1)\n\n[1] 0.7228955 0.6598094\n\nSo now we can trace out the QRE (as a function of choice precision \\(\\lambda\\)) for the three treatments:\n\n\nQREdata<-tibble()\n\nlambda<-1/(1-seq(0,0.99999,length=100))-1\n\nfor (cc in c(0,2,4)) { \n  for (ll in lambda) {\n    p<-QRE(cc,ll)\n    tmp<-tibble(pH=p[1],pA=p[2],lambda=ll,c=cc,ctxt = paste(\"attack cost =\",cc))\n    QREdata<-rbind(QREdata,tmp)\n  }\n}\n\nhead(QREdata)\n\n# A tibble: 6 x 5\n     pH    pA lambda     c ctxt           \n  <dbl> <dbl>  <dbl> <dbl> <chr>          \n1 0.5   0.5   0          0 attack cost = 0\n2 0.500 0.505 0.0102     0 attack cost = 0\n3 0.500 0.510 0.0206     0 attack cost = 0\n4 0.501 0.516 0.0312     0 attack cost = 0\n5 0.501 0.521 0.0421     0 attack cost = 0\n6 0.502 0.526 0.0532     0 attack cost = 0\n\n\n\nd<-d %>% mutate(ctxt =paste(\"attack cost =\",cost) )\n\n(\n  ggplot(data=QREdata,aes(x=pA,y=pH))\n  +geom_path()\n  +theme_bw()\n  +geom_point(data=d,aes(x=attack,y=alert))\n  +facet_wrap(~ctxt)\n  +xlab(\"Attack probability\")\n  +ylab(\"High alert probability\")\n  +xlim(c(0,1))+ylim(c(0,1))\n) \n\n\n\nNote that for the medium attack cost, there appears to be no QRE prediction. This is because \\((p_H,p_A)=(0.5,0.5)\\) for all \\(\\lambda\\) in this case. Risk-neutral QRE provides no additional predictions beyone Nash equilibrium (or coin-flipping, for that matter) in this case.\nEstimating the model in Stan\nLet’s estimate the choice precision parameter \\(\\lambda\\). To do this, we will first need to specify the likelihood. This is fairly straightforward, as the model specifies a probability distribution over actions:\n\\[\n\\begin{aligned}\n\\mathcal p(y\\mid \\lambda)&=\\prod_{c\\in\\{0,2,4\\}}p_A^c(\\lambda)^{A_c}(1-p_A^c(\\lambda))^{N_c-A_c}p_H^c(\\lambda)^{H_c}(1-p_H^c(\\lambda))^{N_c-H_c}\\\\\n\\log p(y\\mid\\lambda)&=\\sum_{c\\in\\{0,2,4\\}}\\left[\nA_c\\log(p_A^c(\\lambda))+(N_c-A_c)\\log(1-p^c_A(\\lambda))\n+H_c\\log(p_H^c(\\lambda))+(N_c-H_c)\\log(1-p^c_H(\\lambda))\n\\right]\n\\end{aligned}\n\\] where \\(N_c\\) is the number of rounds played in treatment \\(c\\), \\(A_c\\) is the number of times “Attack” was played in treatment \\(c\\), and \\(H_c\\) is the number of times “High alert” was played in period \\(c\\).\nWhile this is the easy part, the part I found not so easy in implementing this was to actually solve for the QRE. Remember that we cannot do this by hand, so we will need to pass instructions to Stan on how to do this. Fortunately Stan has a nonlinear equation solver, so in principle this is doable. What I found in practice in writing my working paper (see above), is that if you are willing to do a bit of math beforehand, you can make the problem a lot simpler for Stan to compute. I will not go into this here, but I encourage you to learn about the predictor-corrector algorithm in:\n\nTurocy, Theodore L. “Computing sequential equilibria using agent quantal response equilibria.” Economic Theory 42, no. 1 (2010): 255-269.\n\nWhat I ended up doing in my working paper was just using corrector steps (as my problem was relatively stable).\nBut for now, let’s make Stan do all of the heavy lifting, because this problem is a relatively quick one to solve, particularly since all of the information needed to compute the likelihood is in the following three-row table:\n\n\ndsummary<-(d\n  %>% group_by(cost)\n  %>% summarize(H = sum(alert*15),\n                A = sum(attack*15),\n                N = n()*15 # Each player played 15 rounds in the final 15 rounds\n                )\n)\n\ndsummary %>% knitr::kable()\n\ncost\nH\nA\nN\n0\n120.15\n72.60\n180\n2\n99.00\n47.10\n180\n4\n43.05\n31.05\n180\n\n(yes, I’m noting some rounding errors here. Let’s just go with it).\nHere is the Stan program I wrote to estimate this QRE:\n\nfunctions {\n  \n  \n  /*\n  QRE fixed point condition\n  This function is equal to zero when evaluated at the logit of the QRE probabilities\n  Stan requires that functions passed to the algebra solver have a certain signature, so\n  I had to add in the x_r and x_i inputs to get it to work, even though I don't\n  need these\n  */\n  vector QREFixedPoint(vector lp, vector pars,  data real[] x_r, data int[] x_i) {\n    vector[2] z;\n    \n    real lambda = pars[1]; // lambda is the first element of pars\n    real cost = pars[2];   // attack cost is the second element of pars\n    \n    // convert logit probabilities into actual probabilities\n    vector[2] p = 1.0 ./(1.0+exp(-lp)); \n    \n    // fixed point condition for row player\n    z[1] = lp[1]-lambda*(6*p[2]-3);\n    // fixed point condition for column player\n    z[2] = lp[2]-lambda*(6*(1-p[1])-cost-1);\n    \n    return z;\n    \n  }\n}\n\ndata {\n  int<lower=0> n; // number of rows of data\n  vector[n] cost; // attack cost\n  vector[n] H;    // number of times High Alert is played in a treatment\n  vector[n] A;    // Number of times Attack is played\n  vector[n] N;    // total number of actions played\n  \n  vector[2] prior_lambda; // prior for lambda (lognormal)\n}\n\ntransformed data {\n  // The algebra solver needs to have some data to pass to it, so let's greate some\n  real x_r[1] = {3.14};\n  int x_i[1]  = {42};\n}\n\nparameters {\n  // logit choice precision parameter\n  real<lower=0> lambda;\n  \n}\n\ntransformed parameters {\n  \n  /* sometimes we need to store some intermediate values from our calculations.\n  In particular here, I want to store the model's predictions to plot later.\n  I also store the log-likelihood, as it is useful for model evaluation ( although\n  I', not doing one here).\n  */\n  vector[n] log_like;\n  vector[2] predictions[n];\n  for (tt in 1:n) {\n    \n    // set up some inputs for the fixed point calculation\n    vector[2] lp0 = to_vector({0.0,0.0}); // initial guess\n    vector[2] pars = to_vector({lambda,cost[tt]}); // parameters to pass to solver\n    \n    // solve QRE \n    vector[2] lp = algebra_solver(QREFixedPoint,lp0,pars,x_r,x_i);\n    \n    // convert logit probabilities to actual probabilities\n    vector[2] p = 1.0 ./(1+exp(-lp));\n    predictions[tt] = p;\n    \n    \n    // likelihood contribution\n    log_like[tt]=  H[tt]*log(p[1])+(N[tt]-H[tt])*log(1.0-p[1])\n            + A[tt]*log(p[2])+(N[tt]-A[tt])*log(1.0-p[2]);\n    \n  }\n}\n\n\nmodel {\n  \n  // prior\n  lambda ~ lognormal(prior_lambda[1],prior_lambda[2]);\n  \n  // likelihood contribution\n  target+=log_like;\n  \n  \n}\n\nAnd then we just need to get our data into the raight shape for Stan to understand it:\n\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nStanData<-list( \n  n    = dim(dsummary)[1],\n  cost = dsummary$cost,\n  A    = dsummary$A,\n  H    = dsummary$H,\n  N    = dsummary$N, \n  prior_lambda = c(0,1)\n)\n\nFit<-stan(\"HSS_QREEstimator.stan\",data=StanData)\n\nsummary(Fit)$summary %>% knitr::kable()  \n\n\nmean\nse_mean\nsd\n2.5%\n25%\n50%\n75%\n97.5%\nn_eff\nRhat\nlambda\n1.0670076\n0.0059422\n0.2137486\n0.7220692\n0.9204513\n1.0409649\n1.1853911\n1.5545090\n1293.948081\n1.0014610\nlog_like[1]\n-261.6190865\n0.0099209\n0.3598706\n-261.9810071\n-261.8953710\n-261.7232159\n-261.4495853\n-260.6983400\n1315.810402\n1.0015946\nlog_like[2]\n-249.5329850\n0.0000000\n0.0000000\n-249.5329850\n-249.5329850\n-249.5329850\n-249.5329850\n-249.5329850\n2.003004\n0.9989995\nlog_like[3]\n-195.7950590\n0.0259065\n1.0000421\n-198.6174376\n-196.0307494\n-195.4062060\n-195.1584202\n-195.0897782\n1490.107677\n1.0009788\npredictions[1,1]\n0.7275373\n0.0007128\n0.0258738\n0.6731964\n0.7108820\n0.7285241\n0.7455747\n0.7750579\n1317.572410\n1.0014121\npredictions[1,2]\n0.6558632\n0.0002583\n0.0093302\n0.6326351\n0.6511664\n0.6580500\n0.6629013\n0.6666792\n1304.276919\n1.0015406\npredictions[2,1]\n0.5000000\nNaN\n0.0000000\n0.5000000\n0.5000000\n0.5000000\n0.5000000\n0.5000000\nNaN\nNaN\npredictions[2,2]\n0.5000000\nNaN\n0.0000000\n0.5000000\n0.5000000\n0.5000000\n0.5000000\n0.5000000\nNaN\nNaN\npredictions[3,1]\n0.2724627\n0.0007128\n0.0258738\n0.2249421\n0.2544253\n0.2714759\n0.2891180\n0.3268036\n1317.572410\n1.0014121\npredictions[3,2]\n0.3441368\n0.0002583\n0.0093302\n0.3333208\n0.3370987\n0.3419500\n0.3488336\n0.3673649\n1304.276919\n1.0015406\nlp__\n-706.9669551\n0.0196114\n0.7864392\n-709.2127305\n-707.1427199\n-706.6617103\n-706.4689288\n-706.4151604\n1608.102455\n1.0006512\n\nMy mean estimate of \\(\\lambda=1.053\\) is near enough (e.g. within much less of a standard deviation) to the paper’s maximum likelihood estimate of \\(\\lambda = 0.9782 \\ (0.1618)\\). This should not be too surprising, because with 180 decisions per treatment, the likelihood will be influencing my results much more than the prior.\nDoing something with the model\nSo why do I bother to implement my work using Bayesian techniques when MLE often works just fine? There are few reasons. One is that not relevant here is that a lot of my work involves handling unobservable heterogeneity, which is really easy to implement in Bayes. But another benefit is that once you have draws from the posterior distribution of your parameteres (in this case just \\(\\lambda\\)), you can easily simulate the posterior distribution of predictions of the model. For this example, I did this directly in the “transformed parameters” block of my Stan code, which among other things calculates the QRE choice probabilities. What if we wanted to show these predictions in a table, including an expression of uncertainty. In a frequentist MLE framework, we would have to do something like:\nCarry through our standard errors of our parameters into this nonlinear transformation. This could involve the delta method approximation, which since we are making a nonlinear transformation may require computing a derivative by hand, or\nBootstrap the standard error, which for more complicated QRE models can be computationally intensive.\nOn the other hand, with the posterior simulation in hand, we can just report the mean and standard deviation of our simulated variables, which Stan nicely calculated for us in the summary table above. Note that since the QRE for medium attack cost is always \\(p=(0.5,0.5)\\) for all \\(\\lambda\\), there is no uncertainty about the model’s predictions, even becore we estimate the model.\n\n\n\n",
    "preview": "posts/2022-07-14-estimating-qre/estimating-qre_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2025-09-20T08:31:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-14-visualizing-the-ohio-covid-19-data/",
    "title": "Visualizing the Ohio COVID-19 data",
    "description": {},
    "author": [
      {
        "name": "James Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2020-05-19",
    "categories": [],
    "contents": "\r\nWhile I have been waiting for my Stan programs to run, I have been playing around with Ohio’s COVID-19 datset. So here are a few plots that I have made while learning a bit more R and dplyr.\r\n\r\n\r\nrm(list = ls())\r\nlibrary(tidyr)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(gganimate)\r\nlibrary(lubridate)\r\nlibrary(\"survminer\")\r\nrequire(survival)\r\nlibrary(xtable)\r\nlibrary(scales)\r\nset.seed(42)\r\n\r\n# Source: https://worldpopulationreview.com/us-counties/oh/\r\npop<-read.csv(\"CountyPop.csv\")\r\n\r\npop$CTYNAME<-gsub(\" .*\",\"\",pop$CTYNAME)\r\n\r\ncolnames(pop)<-c(\"County\",\"Population\",\"GrowthRate\")\r\n\r\n\r\n\r\nD<-data.frame(read.csv('https://coronavirus.ohio.gov/static/COVIDSummaryData.csv'))\r\n\r\ncolnames(D)[1]<-c(\"County\")\r\n\r\nD<-merge(D,pop,by=c(\"County\"))\r\n\r\n\r\n# convert dates to days since 2020-01-01\r\nD$Onset.Date<-yday(as.Date(D$Onset.Date, format=\"%m/%d/%Y\"))-1\r\nD$Admission.Date<-yday(as.Date(D$Admission.Date, format=\"%m/%d/%Y\"))-1\r\nD$Date.Of.Death<-yday(as.Date(D$Date.Of.Death, format=\"%m/%d/%Y\"))-1\r\ncolnames(D)[3]<-\"Age\"\r\n\r\nknitr::kable(head(D))\r\nCounty\r\nSex\r\nAge\r\nOnset.Date\r\nDate.Of.Death\r\nAdmission.Date\r\nCase.Count\r\nDeath.Count\r\nHospitalized.Count\r\nPopulation\r\nGrowthRate\r\nAdams\r\nFemale\r\n60-69\r\n78\r\nNA\r\nNA\r\n1\r\n0\r\n0\r\n27724\r\n-2.9068\r\nAdams\r\nMale\r\n40-49\r\n88\r\nNA\r\nNA\r\n1\r\n0\r\n0\r\n27724\r\n-2.9068\r\nAdams\r\nFemale\r\n0-19\r\n118\r\nNA\r\nNA\r\n1\r\n0\r\n0\r\n27724\r\n-2.9068\r\nAdams\r\nMale\r\n80+\r\n128\r\n137\r\n132\r\n1\r\n1\r\n1\r\n27724\r\n-2.9068\r\nAdams\r\nMale\r\n20-29\r\n131\r\nNA\r\nNA\r\n1\r\n0\r\n0\r\n27724\r\n-2.9068\r\nAdams\r\nMale\r\n40-49\r\n105\r\nNA\r\nNA\r\n1\r\n0\r\n0\r\n27724\r\n-2.9068\r\n\r\nPolicies\r\nHere I compile a (very incomplete) list of policies, directives, orders, and so on of things happening at the state and federal level, mostly based on the Stay Safe Ohio Order.\r\n\r\n\r\n# Stay Safe Order Ohio\r\n#https://coronavirus.ohio.gov/static/publicorders/Directors-Stay-Safe-Ohio-Order.pdf\r\nDates<-rbind(c(\"03/16/2020\",\"President Trump's Coronavirus guidelines issued\",\"Guidelines\",\"Trump\",\"closure\"),\r\n             c(\"03/15/2020\",\"Ohio Department of Health: limits access to jails, limits sale of food to carry-out and delivery only\",\"Jails,  Carryout\",\"State\",\"closure\"),\r\n             c(\"03/15/2020\",\"CDC issues Interim Guidance for mass gatherings (>50 people)\",\"Gatherings > 50\",\"CDC\",\"closure\"),\r\n             c(\"03/16/2020\",\"Ohio department of health closes polling stations for March 17 primary election\",\"Polls closed\",\"State\",\"closure\"),\r\n             c(\"03/17/2020\",\"Ohio Department of Health limits mass gatherings\",\"gatherings\",\"State\",\"closure\"),\r\n             c(\"03/19/2020\",\"Ohio Department of Health closes hair salons\",\"Salons\",\"State\",\"closure\"),\r\n             c(\"03/21/2020\",\"Ohio Department of Health closes older adult day care and family entertainment centers\",\"Adult day care\",\"State\",\"closure\"),\r\n             c(\"03/22/2020\",\"Ohio Department of Health orders all persons to stay at home unless engaged in essential activity\",\"Non essential stay at home\",\"State\",\"closure\"),\r\n             c(\"03/24/2020\",\"Ohio Department of Health closes child care services\",\"Child care\",\"State\",\"closure\"),\r\n             c(\"03/30/2020\",\"Ohio Department of Health closes K-12 schools\",\"K-12\",\"State\",\"closure\"),\r\n             \r\n             c(\"03/09/2020\",\"Governor declares state of emergency (Executive order 2020-01D)\",\"State of emergency\",\"State\",\"closure\"),\r\n             c(\"04/30/2020\",\"SSOO(9) Non-essential medical services restrictions rescinded\",\"Non-essential medical\",\"State\",\"opening\"),\r\n             c(\"05/04/2020\",\"SSOO(10-11) Non-essential manufacturing, general office environments may re-open\",\"Non-essential manufacturing & office\",\"State\",\"opening\"),\r\n             c(\"05/01/2020\",\"SSOO(12) Retail may re-open: Curbside pickup, delivery, and appointments only\",\"Retail  curbside pickup\",\"State\",\"opening\"),\r\n             c(\"05/12/2020\",\"SSOO(12) Retail may re-open\",\"Retail in store\",\"State\",\"opening\")\r\n)\r\ncolnames(Dates)<-c(\"Date\",\"Event\",\"Short\",\"Group\",\"Type\")\r\n\r\nDates<-data.frame(Dates)\r\n\r\n\r\n\r\n\r\nDates$Date.Int<-yday(as.Date(Dates$Date, format=\"%m/%d/%Y\"))-1\r\n\r\n#Dates<-Dates %>% arrange(Date.Int)\r\nDates$position<-((1:dim(Dates)[1])-floor((1:dim(Dates)[1])/4)*4)*(-1)^(1:dim(Dates)[1])/4\r\nDates$position<-runif(dim(Dates)[2])*(-1)^(1:dim(Dates)[1])\r\nknitr::kable(Dates)\r\nDate\r\nEvent\r\nShort\r\nGroup\r\nType\r\nDate.Int\r\nposition\r\n03/16/2020\r\nPresident Trump’s Coronavirus guidelines issued\r\nGuidelines\r\nTrump\r\nclosure\r\n75\r\n-0.9148060\r\n03/15/2020\r\nOhio Department of Health: limits access to jails, limits sale of food to carry-out and delivery only\r\nJails, Carryout\r\nState\r\nclosure\r\n74\r\n0.9370754\r\n03/15/2020\r\nCDC issues Interim Guidance for mass gatherings (>50 people)\r\nGatherings > 50\r\nCDC\r\nclosure\r\n74\r\n-0.2861395\r\n03/16/2020\r\nOhio department of health closes polling stations for March 17 primary election\r\nPolls closed\r\nState\r\nclosure\r\n75\r\n0.8304476\r\n03/17/2020\r\nOhio Department of Health limits mass gatherings\r\ngatherings\r\nState\r\nclosure\r\n76\r\n-0.6417455\r\n03/19/2020\r\nOhio Department of Health closes hair salons\r\nSalons\r\nState\r\nclosure\r\n78\r\n0.5190959\r\n03/21/2020\r\nOhio Department of Health closes older adult day care and family entertainment centers\r\nAdult day care\r\nState\r\nclosure\r\n80\r\n-0.7365883\r\n03/22/2020\r\nOhio Department of Health orders all persons to stay at home unless engaged in essential activity\r\nNon essential stay at home\r\nState\r\nclosure\r\n81\r\n0.9148060\r\n03/24/2020\r\nOhio Department of Health closes child care services\r\nChild care\r\nState\r\nclosure\r\n83\r\n-0.9370754\r\n03/30/2020\r\nOhio Department of Health closes K-12 schools\r\nK-12\r\nState\r\nclosure\r\n89\r\n0.2861395\r\n03/09/2020\r\nGovernor declares state of emergency (Executive order 2020-01D)\r\nState of emergency\r\nState\r\nclosure\r\n68\r\n-0.8304476\r\n04/30/2020\r\nSSOO(9) Non-essential medical services restrictions rescinded\r\nNon-essential medical\r\nState\r\nopening\r\n120\r\n0.6417455\r\n05/04/2020\r\nSSOO(10-11) Non-essential manufacturing, general office environments may re-open\r\nNon-essential manufacturing & office\r\nState\r\nopening\r\n124\r\n-0.5190959\r\n05/01/2020\r\nSSOO(12) Retail may re-open: Curbside pickup, delivery, and appointments only\r\nRetail curbside pickup\r\nState\r\nopening\r\n121\r\n0.7365883\r\n05/12/2020\r\nSSOO(12) Retail may re-open\r\nRetail in store\r\nState\r\nopening\r\n132\r\n-0.9148060\r\n\r\nEvent counts by day\r\n\r\n\r\n## new cases\r\nplt<-(\r\n  ggplot()\r\n  +theme_bw()\r\n  +geom_histogram(data=D[D$Onset.Date>=50,],aes(x=Onset.Date),binwidth=1)\r\n  +ylab(\"Daily cases\")+xlab(\"Days since 2020-01-01\")\r\n  +geom_segment(data=Dates,aes(y=400*position,yend=0,x=Date.Int,xend=Date.Int,color=Type,linetype=Group))\r\n  +geom_text(data=Dates,aes(y=400*position,x=Date.Int,label=Short,color=Type),size=3)\r\n  #+coord_fixed(ratio = 1e-2)\r\n)\r\nplt\r\n\r\n\r\n\r\n\r\n## Admitted to hospital\r\nplt<-(\r\n  ggplot(data=D,aes(x=Admission.Date))\r\n  +theme_bw()\r\n  +geom_histogram(binwidth=1)\r\n  +geom_segment(data=Dates,aes(y=100*position,yend=0,x=Date.Int,xend=Date.Int,color=Type,linetype=Group))\r\n  +geom_text(data=Dates,aes(y=100*position,x=Date.Int,label=Short,color=Type),size=3)\r\n  +ylab(\"Hospital admissions\")+xlab(\"Days since 2020-01-01\")\r\n)\r\nplt\r\n\r\n\r\n\r\n\r\n## Admitted to hospital\r\nplt<-(\r\n  ggplot(data=D,aes(x=Date.Of.Death))\r\n  +theme_bw()\r\n  +geom_histogram(binwidth=1)\r\n  +ylab(\"Deaths\")+xlab(\"Days since 2020-01-01\")\r\n  +geom_segment(data=Dates,aes(y=60*position,yend=0,x=Date.Int,xend=Date.Int,color=Type,linetype=Group))\r\n  +geom_text(data=Dates,aes(y=60*position,x=Date.Int,label=Short,color=Type),size=3)\r\n)\r\nplt\r\n\r\n\r\nSurvival\r\nProbably very pessimistic due to tests being conditioned on symptoms.\r\n\r\n\r\nD$Died<-1+as.integer(!is.na(D$Date.Of.Death))\r\nD$time<-max(D$Onset.Date)-D$Onset.Date\r\n\r\nD<-D[order(D$Age),]\r\n\r\nfit <- survfit(Surv(time, Died) ~ Age,data=D)\r\n\r\n\r\nggsurvplot(fit, data = D,conf.int = TRUE, ggtheme = theme_bw() ,\r\n           xlab=\"Time since onset date (days)\")\r\n\r\n\r\nBy county\r\n\r\n\r\nCases<-group_by(D, County,Onset.Date) %>% summarise(New.Cases= length(time))\r\ncolnames(Cases)[2]<-\"Date\"\r\n\r\nAdmissions<-group_by(D[!is.na(D$Admission.Date),], County,Admission.Date) %>% summarise(New.Admissions= length(time))\r\ncolnames(Admissions)[2]<-\"Date\"\r\n\r\n\r\nDeaths<-group_by(D[!is.na(D$Date.Of.Death),], County,Date.Of.Death) %>% summarise(New.Deaths= length(time))\r\ncolnames(Deaths)[2]<-\"Date\"\r\n\r\nCOUNTIES<-merge(Cases,merge(Deaths,Admissions,by=c(\"County\",\"Date\"),all=TRUE),by=c(\"County\",\"Date\"),all=TRUE)\r\n\r\nFirst.Case.Date<-aggregate(COUNTIES$Date,by=list(COUNTIES$County),FUN=min)\r\ncolnames(First.Case.Date)[1]<-\"County\"\r\nDateList<-1:max(D$Onset.Date)\r\ndl<-data.frame(DateList)\r\ndl$Date<-dl$DateList\r\n\r\nDL<-data.frame()\r\nCList<-unique(COUNTIES$County)\r\nfor (cc in 1:length(CList)) {\r\n  tmp<-dl\r\n  tmp$County<-CList[cc]\r\n  DL<-rbind(DL,tmp)\r\n}\r\n\r\nCOUNTIES<-merge(COUNTIES,DL[,c(2,3)],by=c(\"County\",\"Date\"),all=TRUE)\r\nCOUNTIES<-COUNTIES[order(COUNTIES$County,COUNTIES$Date),]\r\n\r\n# NA==> no death/admission/case on that day\r\nCOUNTIES[is.na(COUNTIES)]<-0\r\n\r\nCOUNTIES<-merge(COUNTIES,pop[,c(\"County\",\"Population\")],by=\"County\")\r\nCOUNTIES<-COUNTIES %>%\r\n          arrange(County,Date) %>%\r\n          group_by(County) %>%\r\n          mutate(Cumulative.Cases = cumsum(New.Cases)) %>%\r\n          mutate(Cumulative.Deaths = cumsum(New.Deaths)) %>%\r\n          mutate(Cumulative.Admissions = cumsum(New.Admissions)) %>%\r\n          mutate(Cumulative.Cases.Frac = Cumulative.Cases/Population) %>%\r\n          mutate(Cumulative.Deaths.Frac = Cumulative.Deaths/Population) %>%\r\n          mutate(Cumulative.Admissions.Frac = Cumulative.Admissions/Population)\r\n\r\nCOUNTIES<-merge(COUNTIES,First.Case.Date,by=\"County\")\r\n\r\nCOUNTIES$Time.Since.First.Case<-COUNTIES$Date-COUNTIES$x\r\n\r\n\r\n\r\n(ggplot(COUNTIES[COUNTIES$Time.Since.First.Case>=0,],aes(x=Time.Since.First.Case,y=Cumulative.Cases.Frac*100000,group=County,color=Population))\r\n      +geom_line()\r\n      +theme_bw()\r\n      +xlab(\"Days since first case\")\r\n      +ylab(\"Cases per 100,000\")\r\n      +scale_y_continuous(trans = \"log10\")\r\n)\r\n\r\n\r\n\r\n\r\n(ggplot(COUNTIES[COUNTIES$Time.Since.First.Case>=0,],aes(x=Time.Since.First.Case,y=Cumulative.Deaths.Frac*100000,group=County,color=Population))\r\n      +geom_line()\r\n      +theme_bw()\r\n      +xlab(\"Days since first case\")\r\n      +ylab(\"Deaths per 100,000\")\r\n      +scale_y_continuous(trans = \"log10\")\r\n)\r\n\r\n\r\n\r\n\r\nplt<-(ggplot(COUNTIES[COUNTIES$Time.Since.First.Case>=0,],aes(y=Cumulative.Deaths.Frac*100000,x=Cumulative.Cases.Frac*100000,label=County,size=Population,color=Cumulative.Admissions.Frac*100000))\r\n      +geom_point()\r\n      +theme_bw()\r\n      #+theme(legend.position = \"none\")\r\n      +transition_time(Time.Since.First.Case)\r\n      +labs(title= \"COVID-19 cases by county in Ohio\",subtitle = \"Days since first case {round(frame_time, 0)}\")\r\n      +xlab(\"Cases per 100,000\")\r\n      +ylab(\"Deaths per 100,000\")\r\n      +scale_y_continuous(trans = \"log10\")\r\n      +scale_x_continuous(trans = \"log10\")\r\n      +labs(color = \"Admissions per 100,000\")\r\n      +scale_fill_gradient(\r\n        low = \"blue\",\r\n        high = \"red\",\r\n        space = \"Lab\",\r\n        na.value = \"grey50\",\r\n        guide = \"colourbar\",\r\n        aesthetics = \"color\"\r\n      )\r\n)\r\nanimate(plt,nframes=max(COUNTIES$Time.Since.First.Case))\r\n\r\n\r\n\r\n\r\nCountiesAggregate<- COUNTIES %>% group_by(County) %>%\r\n            filter(Cumulative.Deaths.Frac == max(Cumulative.Deaths.Frac),\r\n                   Cumulative.Cases.Frac == max(Cumulative.Cases.Frac),\r\n                   Cumulative.Admissions.Frac == max(Cumulative.Admissions.Frac))\r\n\r\n(ggplot(data=CountiesAggregate,aes(x=Cumulative.Cases.Frac*10^5,y=Cumulative.Deaths.Frac*10^5,label=County))\r\n      +scale_y_continuous(trans = \"log10\")\r\n      +scale_x_continuous(trans = \"log10\")\r\n      +geom_text(size=2)\r\n      +xlab(\"Cases per 100,000\")\r\n      +ylab(\"Deaths per 100,000\")\r\n      +theme_bw()\r\n      +geom_smooth(method=lm,formula=y~x)#\r\n      +labs(title = \"COVID-19 cases and deaths by county in Ohio\",subtitle = paste(\"Produced on\",Sys.Date()))\r\n  \r\n)\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-05-14-visualizing-the-ohio-covid-19-data/visualizing-the-ohio-covid-19-data_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-09-20T08:31:29-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-11-longyearbyendrycooking/",
    "title": "Pizza, beer, potatoes, and tacos at 78° North",
    "description": "How a town's unhealthy relationship with late-night pizza was remedied by a potato stand.",
    "author": [
      {
        "name": "James Bland",
        "url": {}
      },
      {
        "name": "Emma Bland",
        "url": "https://www.unis.no/staff/emma-bland/"
      }
    ],
    "date": "2020-03-14",
    "categories": [],
    "contents": "\r\nOver spring break, I had the pleasure of visiting my sister, Emma Bland in Longyearbyen, Svalbard. As much as this trip was supposed to be about catching up with my sister, and messing with the all sky camera at the observatory:\r\n\r\n\r\nMy sister does stuff with radar at the north pole. She also knows when* the all sky camera at the observatory is taking photos * it's every 20 seconds, so this was not that hard pic.twitter.com/WsrB7BJcc8\r\n\r\n— James Bland (@JamesBland_Econ) March 10, 2020\r\n\r\nI couldn’t help nerding out with this little story about negative externalities, and a solution to the problem involving street food.\r\nPre-2019 Longyearbyen\r\nLongyearbyen is a town in the high Arctic. Historically, it has had strategic and coal-mining roots, but this has mostly given way to tourism and research. Temperatures are often way below freezing, and if one is leaving town, one has to take a rifle with them in case there is a polar bear. My sister, Dr. Emma Bland, is a postdoc at the University Center in Svalbard, and when she’s not studying the aurora she spends her time exploring nearby mountains, glaciers, and ice caves by foot, skis, and snowmobile.\r\nDespite its remote location, the tourism industry ensures that Longyearbyen has a vibrant restaurant and bar scene. As such, it is not uncommon for people to be walking home from a merry evening on the town in the wee hours of the morning. What does one frequently crave after a walk home from the pub? Well, this can be somewhat location-dependent. For example, in my hometown of Melbourne, it is traditional to satisfy one’s late-night cravings with a kebab:\r\n\r\n\r\nHowever, it turns out that less-than-sober Norwegians’ tastes are more geared towards “Italian” food, so the aforementioned ambulations to one’s domicile are more likely accompanied by a pizza. In fact, given that there are no pizza places open during customary post-drinking times, these pizzas are most likely frozen pizzas, cooked in the comfort of one’s own home.\r\nThis is where things start to become a bit dangerous. A frozen pizza takes some time to cook, and since our merry protagonist is getting a bit drowsy by this point in the evening, it is not uncommon for people to fall asleep before the pizza is ready, and then get woken up by the fire department when the pizza is a bit on the well-done side of charred and smoking.\r\nFire is a very serious issue for Longyearbyen. Most people live in closely-spaced wooden apartments, so the consequences of a fire are rarely isolated to one home. Additionally, smoke detectors automatically call the fire department, and so these activities not only put quite a few people in danger, but are also a drain on emergency responders’ resources.\r\nLet’s have a think about this from a Principles of Microeconomics standpoint. Consumption of frozen pizzas in Longyearbyen imposes a negative externality on our late-night chef’s neighbors. Hence, the supply curve does not measure the full social marginal cost of the good:\r\n\r\n\r\n\r\nTherefore, frozen pizza consumption is too high: since consumers and producers do not bear the entire cost of frozen pizzas, the market encourages too much late-night pizza cooking from society’s standpoint.\r\nA more usual Principles of Micro solution to this would be to impose a tax on frozen pizzas, which makes consumers pay the full social cost of their activities. However, sometimes by happy accident (or unhappy accident, if we’re talking about Jurassic Park), life finds a way.\r\nA partial fix\r\nThis all changed in 2019, when a potato stand opened up in downtown Lonyearbyen. This stand is currently open between 10pm and 4am, Friday-Sunday. So people are much less likely to be hungry when they get home from the pub. In addition to this, there is also a taco truck open Wednesday-Saturday fulfilling this need. Tacos and potatoes are substitutes for frozen pizzas, so having these food trucks available late at night means that demand for frozen pizzas has shifted to the left, and the fire department’s dreaded “Tørrkoking” (“dry cooking”) callouts are on the way down.\r\n\r\n\r\n\r\nAs the local English-language newspaper puts it:\r\n\r\nKjelleberg’s [potato] truck– and therefore likely Styrsell’s [taco truck] – is satisfying more than just people looking for something different – or anything at all in the wee-morning hours when pubs close and all other eateries are closed. The Longyearbyen Fire Department praised Kjelleberg for providing such an option because it reduced the likelihood of intoxicated people igniting “dry cooking” fires by falling asleep while trying to cook food at home, which the department has called its top threat the past several years.\r\n\r\nWhile there really aren’t enough years to do a propper analysis with (say) synthetic controls, it looks like these food trucks are bringing more than just tasty food to the community.\r\n\r\n\r\n\r\nTo paraphrase Adam Smith:\r\n\r\nIt is not from the benevolence of the potato vendor or the taco truck owner, that we expect to safely satisfy our late-night cravings, but from their regard to their own interest. We address ourselves, not to their humanity but to their self-love, and never talk to them of our own necessities but of their advantages.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-03-11-longyearbyendrycooking/longyearbyendrycooking_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2025-09-20T08:31:27-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-02-20200302regressiondiscontinuity/",
    "title": "Simulating a problem with regression discontinuity, and then understanding (part of) it with a DAG",
    "description": "When people get to choose their assignment variable",
    "author": [
      {
        "name": "James Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2020-03-03",
    "categories": [],
    "contents": "\r\nFor the last couple of years, I have given my econometrics class (ECON 5820) a simulation exercise problem set for regression discontinuity. (see Exercise 10.5 of my econometrics notes for the whole thing). This was the first year for me teaching Directed Acyclic Graphs, and my students were learning about these while doing this problem set. To my delight, we went off on a bit of a tangent today by putting them together.\r\nThe point of this exercise is to use a simulation to show that if people can manipulate their assignment variable, then the estimator is biased. Here is the problem:\r\n\r\nAt the beginning of the semester, students take a test (\\(X_1\\)) to determine whether they take an advanced class (\\(D=1\\)) or not \\(D=0\\). If they score above \\(X=75\\) on the test, they take the advanced class, otherwise they do not. However, if they score between 70 and 75, they may re-take the test (\\(X_2\\)), and take the advanced class if they score above 75 on the retake. At the end of the semester, they take a final test (\\(Y\\)) to assess understanding of the material. Using a regression discontinuity design, you estimate the causal effect of being in the advanced class (\\(D\\)) on this final test score (\\(Y\\)).\r\n\r\nIf you are unfamiliar with regression discontinuity design and/or directed acyclic graphs, a great place to start is Scott Cunningham’s Causal Inference: The Mixtape, where there are chapters on both. My class uses the chapter on DAGs, as well as Paul Hünermund’s slides. I mostly teach regression discontinuity design out of Michael Bailey’s “Real Econometrics”.\r\nI ask them to simulate the distribution of the estimator under two conditions:\r\nNo retakes\r\nRetakes, and we only observe the score of the first test after retakes.\r\nI give students the following data-generating process: \\[\r\n\\begin{aligned}\r\n\\text{ability (unobserved)} &\\sim N(70,2.3^2)\\\\\r\n\\text{first test score: } X_1 &\\sim N(\\mathrm{ability},1.5^2)\\\\\r\n\\text{retake: }X_2&=\\begin{cases}\r\nN(\\mathrm{ability},1.5^2)&\\text{if }X_1\\in(70,75)\\\\\r\nX_1&\\text{otherwise}\r\n\\end{cases}\r\n\\\\\r\n\\text{Advanced class: } D&=\\begin{cases}\r\n1&\\text{if }X_2\\geq 75\\\\\r\n0&\\text{otherwise}\r\n\\end{cases}\\\\\r\n\\text{Final score: } Y&=\\mathrm{ability}+10D\r\n\\end{aligned}\r\n\\]\r\nThat is, the true causal effect of being in the advanced class is 10 points on the final test. Ability is an unobserved confounder.\r\nHere is one draw of data from this distribution:\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(tidyr)\r\nlibrary(dagitty)\r\nset.seed(42) # because it's the answer\r\n\r\nN<-1000 # sample size \r\n\r\nability<-75+rnorm(N)*2.3\r\n\r\nX<-ability+rnorm(N)*1.5\r\nD<-as.integer(X>=75)\r\nY<-ability+10*D\r\n\r\nX2<-X\r\nX2[X>=70 & X<75] <-ability[X>=70 & X<75]+rnorm(sum(X>=70 & X<75))*1.5\r\nD2<-as.integer(X2>=75)\r\nY2<-ability+10*D2\r\n\r\ndf<-data.frame(X,D,Y,X2,D2,Y2)\r\n\r\ndfStack<-data.frame(X,D,Y)\r\ndfStack$retake<-\"no\"\r\ntmp<-data.frame(X2,D2,Y2)\r\ncolnames(tmp)<-c(\"X\",\"D\",\"Y\")\r\ntmp$retake<-\"yes\"\r\ndfStack<-rbind(dfStack,tmp)\r\n\r\nknitr::kable(df[1:10,])\r\nX\r\nD\r\nY\r\nX2\r\nD2\r\nY2\r\n81.64079\r\n1\r\n88.15320\r\n81.64079\r\n1\r\n88.15320\r\n74.48738\r\n0\r\n73.70119\r\n74.07706\r\n0\r\n73.70119\r\n77.29130\r\n1\r\n85.83520\r\n77.29130\r\n1\r\n85.83520\r\n77.02104\r\n1\r\n86.45558\r\n77.02104\r\n1\r\n86.45558\r\n74.43592\r\n0\r\n75.92982\r\n75.51293\r\n1\r\n85.92982\r\n73.85969\r\n0\r\n74.75591\r\n72.16881\r\n0\r\n74.75591\r\n78.72438\r\n1\r\n88.47650\r\n78.72438\r\n1\r\n88.47650\r\n70.38957\r\n0\r\n74.78228\r\n71.77223\r\n0\r\n74.78228\r\n78.37050\r\n1\r\n89.64237\r\n78.37050\r\n1\r\n89.64237\r\n76.05363\r\n1\r\n84.85576\r\n76.05363\r\n1\r\n84.85576\r\n\r\nSo if there are no retakes, we get draws from \\((X,D,Y)\\), and if there are retakes, we get draws from \\((X_2,D_2,Y_2)\\).\r\nAnd here’s how you might be able to spot it in the data (but “eyeball” tests are notoriously bad).\r\n\r\n\r\ncbPalette <- c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\r\nplt<-ggplot(data=dfStack,aes(x=X,color=retake))+geom_density(size=2)\r\nplt<-plt+theme_bw()+scale_colour_manual(values=cbPalette)\r\nplt<-plt+xlab(\"first test score (after retakes)\")+geom_vline(xintercept=75,color=\"black\")\r\nplt \r\n\r\n\r\nNotice that there is a large spike for the “retakes” sample (blue) just above the cutoff, but it might be hard to spot if we don’t have the “no retakes” sample to compare to it.\r\nSimulating the distribution of the estimators\r\nThis is where a simulation comes in handy. Here is code that does this one draw 1,000 times:\r\n\r\n\r\nSimSize<-1000\r\none.trial<-function(){\r\n  N<-1000 # sample size \r\n\r\n  ability<-75+rnorm(N)*2.3\r\n\r\n  X<-ability+rnorm(N)*1.5\r\n  D<-as.integer(X>=75)\r\n  Y<-ability+10*D\r\n\r\n  X2<-X\r\n  II <- X>=70 & X<75\r\n  X2[II] <-ability[II]+rnorm(sum(as.integer(sum(II))))*1.5\r\n  D2<-as.integer(X2>=75)\r\n  Y2<-ability+10*D2\r\n\r\n  df<-data.frame(X,D,Y,X2,D2,Y2)\r\n  \r\n  b1<-lm(Y~D+X,data=df)$coefficients[2]\r\n  b2<-lm(Y2~D2+X2,data=df)$coefficients[2]\r\n  \r\n  c(b1,b2)\r\n}\r\nSim<-replicate(SimSize,one.trial())\r\n\r\ntmp<-data.frame(Sim[1,],Sim[2,])\r\n\r\ncolnames(tmp)<-c(\"D1\",\"D2\")\r\n\r\nD<-tmp$D1\r\nSimD<-data.frame(D)\r\nSimD$retake<-\"no\"\r\n\r\nD<-tmp$D2\r\ntmp2<-data.frame(D)\r\ntmp2$retake<-\"yes\"\r\nSimD<-rbind(SimD,tmp2)\r\n\r\n\r\n\r\nplt<-ggplot(data=SimD,aes(x=D,color=retake))+geom_density(size=2)\r\nplt<-plt+theme_bw()+scale_colour_manual(values=cbPalette)\r\nplt<-plt+xlab(\"Coefficient estimate\")+geom_vline(xintercept=10,color=\"black\")\r\nplt  \r\n\r\n\r\nSo in this instance the estimator is biased upwards if we have re-takes. That is, we would be over-estimating the effect of the advanced class. If you’re familiar with regression discontinuity, this should be unsurprising: one of the things we worry about is that people might get to choose (or in our case, influence) the assignment variable.\r\nUnderstanding the problem with DAGs\r\nThis is where things got more interesting than usual. My students asked whether we could fix the problem by controlling for retakes, if in fact we did observe retakes. So we drew the DAG:\r\n\r\n\r\ng<-dagitty('dag {\r\n  X1[pos=\"0,0\"]\r\n  X2[pos=\"2,-2\"]\r\n  ability[pos=\"2,0\"]\r\n  retake[pos=\"1,-2\"]\r\n  Y[pos=\"1.5,-0.5\"]\r\n  D[pos=\"1.5,-1\"]\r\n  \r\n  ability -> X1\r\n  ability -> X2\r\n  ability -> Y\r\n  X1 -> retake -> X2 -> D -> Y\r\n  X1 -> D\r\n  \r\n}')\r\nplot(g)\r\n\r\n\r\npaths( g, \"D\", \"Y\" )$paths\r\n\r\n[1] \"D -> Y\"                                 \r\n[2] \"D <- X1 -> retake -> X2 <- ability -> Y\"\r\n[3] \"D <- X1 <- ability -> Y\"                \r\n[4] \"D <- X2 <- ability -> Y\"                \r\n[5] \"D <- X2 <- retake <- X1 <- ability -> Y\"\r\n\r\nThe first path, \\(D\\to Y\\) is the causal effect we want to estimate: how much does being in the advanced class increase the final test score? But we need to block paths 2-5. Paths 2, 4, and 5 are blocked already, because we condition on \\(X_2\\) in our regression discontinuity. What we are left with is the following path:\r\n\r\n\r\npaths( g, \"D\", \"Y\" )$paths[3]\r\n\r\n[1] \"D <- X1 <- ability -> Y\"\r\n\r\nSince we do not observe ability, we need to control for \\(X_1\\). Note that it is not sufficient to control for retakes, as this will not block the path.\r\nSuppose now that we observed \\((X_1,X_2,D,Y,\\mathrm{retake})\\) (i.e. we get information on both first and second attempts of the test). Let’s simulate the RDD estimator again, but this time including \\(X_2\\) on the right-hand side as well:\r\n\r\n\r\none.trial<-function(){\r\n  N<-1000 # sample size \r\n\r\n  ability<-75+rnorm(N)*2.3\r\n\r\n  X<-ability+rnorm(N)*1.5\r\n  D<-as.integer(X>=75)\r\n  Y<-ability+10*D\r\n\r\n  X2<-X\r\n  II <- X>=70 & X<75\r\n  X2[II] <-ability[II]+rnorm(sum(as.integer(sum(II))))*1.5\r\n  D2<-as.integer(X2>=75)\r\n  Y2<-ability+10*D2\r\n\r\n  df<-data.frame(X,D,Y,X2,D2,Y2)\r\n  \r\n  lm(Y2~D2+X2+X,data=df)$coefficients[2]\r\n}\r\nD<-as.vector(replicate(SimSize,one.trial()))\r\n\r\n\r\n\r\ntmp<-data.frame(D)\r\ntmp$retake<-\"yes, with X1\"\r\nSimD<-rbind(SimD,tmp)\r\nplt<-ggplot(data=SimD,aes(x=D,color=retake))+geom_density(size=2)\r\nplt<-plt+theme_bw()+scale_colour_manual(values=cbPalette)\r\nplt<-plt+xlab(\"Coefficient estimate\")+geom_vline(xintercept=10,color=\"black\")\r\nplt\r\n\r\n\r\nWhat???!!!?\r\nAccording to the DAG, everything should be peachy. What’s going on? After a bit of head-scratching, my class came to the conclusion that since DAGs only tell us what to put on the RHS of a regression, not the functional form of the RHS, then maybe we need to include some interactions:\r\n\r\n\r\none.trial<-function(){\r\n  N<-1000 # sample size \r\n\r\n  ability<-75+rnorm(N)*2.3\r\n\r\n  X<-ability+rnorm(N)*1.5\r\n  D<-as.integer(X>=75)\r\n  Y<-ability+10*D\r\n\r\n  X2<-X\r\n  II <- X>=70 & X<75\r\n  X2[II] <-ability[II]+rnorm(sum(as.integer(sum(II))))*1.5\r\n  D2<-as.integer(X2>=75)\r\n  Y2<-ability+10*D2\r\n\r\n  df<-data.frame(X,D,Y,X2,D2,Y2)\r\n  \r\n  lm(Y2~D2+X2+X+II+II:X:X2,data=df)$coefficients[2]\r\n}\r\nD<-as.vector(replicate(SimSize,one.trial()))\r\n\r\nHere I am including a retake dummy, as well as an interaction between \\(X_1\\) and \\(X_2\\). Note that the retake dummy is a deterministic function of \\(X_1\\), so this is really just allowing the functional form to be more flexible, and not using any new data.\r\n\r\n\r\ntmp<-data.frame(D)\r\ntmp$retake<-\"yes, with X1 & interactions\"\r\nSimD<-rbind(SimD,tmp)\r\nplt<-ggplot(data=SimD,aes(x=D,color=retake))+geom_density(size=2)\r\nplt<-plt+theme_bw()+scale_colour_manual(values=cbPalette)\r\nplt<-plt+xlab(\"Coefficient estimate\")+geom_vline(xintercept=10,color=\"black\")\r\nplt\r\n\r\n\r\nMuch better!\r\nI am going to be writing all of my posts here from now on, but if you want to see my older posts, they can be found on my google site\r\n\r\n\r\n",
    "preview": "posts/2020-03-02-20200302regressiondiscontinuity/20200302regressiondiscontinuity_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-09-20T08:31:28-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-03-02-computing-pi/",
    "title": "Computing pi using random numbers, and assuming that pi = 2",
    "description": "Throwing simulated darts at a circle within a square",
    "author": [
      {
        "name": "James R Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2020-03-03",
    "categories": [],
    "contents": "\r\nWith pi day just 11 days away, it was fitting that one of my econometrics students read about approximating π using Monte Carlo simulation. Here one way to do it.\r\nImagine you are throwing darts at a square board with side length \\(2r\\). Inside this square is a circle with radius \\(r\\). (here I’m setteing \\(r=1\\)).\r\n\r\n\r\n\r\nIf your darts are uniformly distributed across the board, then the probability that a dart lands in the circle is: \\[\r\n\\Pr\\left(\\text{dart in circle}\\right) = \\frac{\\text{area of circle}}{\\text{area of square}}=\\frac{\\pi r^2}{4r^2}=\\frac{\\pi}{4}\r\n\\]\r\nHence, our approximation of pi is \\(\\hat\\pi = \\frac{4}{S}\\sum_{s=1}^Sc_s\\), where \\(S\\) is the simulation size, and \\(c_s\\) is an indicator variable equal to one if simulated dart \\(s\\) lands in the circle, and zero otherwise.\r\n\r\n\r\nS<-1000000 # simulation size\r\nx<-runif(S)*2-1\r\ny<-runif(S)*2-1\r\nd<-data.frame(x,y)\r\nd$InCircle<-as.integer(sqrt(d$x^2+d$y^2)<1)\r\n\r\npiApprox<-mean(d$InCircle)*4\r\npiApprox\r\n\r\n[1] 3.141012\r\n\r\nCompared to the actual (to my computer’s displayed precision):\r\n\r\n\r\npi\r\n\r\n[1] 3.141593\r\n\r\nAs with any simulation, we’re always going to be at least a little bit off, and maybe it’s even worse than that, especially if the simulation size \\(S\\) is small (although fixing that is a problem for my computer, not me).\r\nHow much benefit do we get by increasing the simulation size? We can answer this by recognizing that variable \\(C_s\\) is destributed \\(\\mathrm{Bernoulli}(\\pi/4)\\), which has mean \\(\\pi/4\\) and variance \\(\\pi/4(1-\\pi/4)\\). And so, using the Lindeberg–Lévy Central Limit Theorem: \\[\r\n\\sqrt{S}(\\hat\\pi-\\pi)\\xrightarrow[]{d} N\\left(0,\\pi/4(1-\\pi/4)\\right)\r\n\\]\r\nNote that since I am approximating \\(\\pi\\), I probably shouldn’t use it’s value to work out the variance of \\(\\hat\\pi\\). Instead, note that \\(p(1-p)\\) is maximized at \\(p=0.5\\), so the asymptotic variance can be at most 0.25 (i.e. if \\(\\pi\\) were exactly two). Hence, the variance of \\(\\hat\\pi\\) must satisfy (approximately, because of the \\(\\xrightarrow[]{d}\\) part): \\[\r\nV[\\hat\\pi]\\leq \\frac{1}{4S}\r\n\\]\r\nSo, for example, if we wanted to have a 95% chance (at least, due to the inequality) of being within 0.0001, we need a simulation size that satisfies:\r\n\\[\r\n\\begin{aligned}\r\n0.0001&=1.96\\sqrt{\\frac{1}{4S}}\\\\\r\n\\left(\\frac{0.0001}{1.96}\\right)^2&=\\frac{1}{4S}\\\\\r\nS&=\\frac{1.96^2}{0.0001^2\\times4}\r\n\\end{aligned}\r\n\\]\r\n\r\n\r\n1.96^2/(0.0001^2*4)\r\n\r\n[1] 96040000\r\n\r\nwhich is almost 100 times larger than the simulation I ran above. This method isn’t particularly efficient!\r\nI am going to be writing all of my posts here from now on, but if you want to see my older posts, they can be found on my google site\r\n\r\n\r\n",
    "preview": "posts/2020-03-02-computing-pi/computing-pi_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2025-09-20T08:31:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-20-afldistance/",
    "title": "Accounting for kicking range in the AFL shot location data",
    "description": {},
    "author": [
      {
        "name": "James R Bland",
        "url": {
          "https://sites.google.com/site/jamesbland/": {}
        }
      }
    ],
    "date": "2020-02-28",
    "categories": [],
    "contents": "\r\nLate last year, I proposed a model for kicking accuracy in Australian Football. This is using data from https://www.statsinsider.com.au/. In particular, I will be using data that goes into their AFL Shot Charting Explorer.\r\nToday I will be extending the model by accounting for a few more categories of data that we have:\r\n\r\n\r\nD<-Data[Data$`Shot type`==\"Set shot\",]\r\nunique(D$`Shot result`)\r\n\r\n\r\n[1] \"Goal\"                 \"Behind\"              \r\n[3] \"Didn't make distance\" \"Miss, out of bounds\" \r\n[5] \"Miss, in play\"        \"Touched\"             \r\n[7] \"Free kick mid-shot\"  \r\n\r\nSo other than “Free kick mid-shot”, which I assume is due to some kind of off-the-play violation, the remaining outcomes are\r\n“Goal” indicates that a goal was scored (6 points)\r\n“Behind” indicates that a behind was scored (1 point)\r\n“Miss, out of bounds” indicates that the kick went the distance, but the angle was off\r\n“Miss, didn’t make the distance” indicates that the kick fell short\r\n“Miss, in play” I am assuming is a similar outcome to “Didn’t make the distance”\r\n“Touched” indicated that the ball was touched before it passed through the goal posts (so it is worth 1 point rather than 6). I will code this the same as “Miss, didn’t make the distance”, as for it to be touched, the ball must have been relatively low to the ground near the goal.\r\nPreviously, I just dropped everything except for the first three outcomes, which is probably a bit dodgy. So here are the empirical accuracies from each of the recorded locations of set shots. Here “distance” is the fraction of shots from that location that made the distance (i.e. went out of bounds or through the goal posts).\r\n\r\n\r\n\r\nLet’s develop a model to account for these outcomes differently.\r\nLet \\(D\\) be a random variable describing the actual distance of a kick (i.e. not the required distance).\r\n\r\n\r\n\r\nTo make the math easier (at the expense of a bit of realism), let’s assume that getting the distance right is not a function of a player’s error in kicking angle (the geometry of this is just wrong, maybe I will incorporate it later). This means that if a player’s kicking distance \\(D\\sim F_D\\), then the “Miss, didn’t make the distance” events occur with probability \\(F_D(r)\\), where \\(r\\) is the distance from the kick to the center of the goal. Hence, the probability of the four possible (mutually exclusive) outcomes that I will be considering are: \\[\r\n\\begin{aligned}\r\n\\Pr(&\\text{Miss, didn't make the distance}\\mid r,\\theta,\\kappa,\\delta,\\nu)\\\\&=F(r;\\delta)\\\\\r\n\\Pr(&\\text{Goal}\\mid r,\\theta,\\kappa,\\delta,\\nu)\\\\&=(1-F(r;\\delta))\\left[1-2\\mathrm t\\left(-\\frac{a(qr)}{2\\kappa},\\nu\\right)\\right]\\\\\r\n%%\r\n\\Pr(&\\text{Behind} \\mid  r,\\theta,\\kappa,\\delta,\\nu)\\\\\r\n&=(1-F(r;\\delta))\\left[\r\n\\mathrm t\\left(-\\frac{a(qr)}{2\\kappa},\\nu\\right)+t\\left(-\\frac{a(qr)}{2\\kappa}-\\frac{a(pq)}{\\kappa},\\nu\\right)\r\nt\\left(\\frac{a(qr)}{2\\kappa}+\\frac{a(rs)}{\\kappa},\\nu\\right) -t\\left(\\frac{a(rs)}{2\\kappa},\\nu\\right)\r\n\\right]\\\\\r\n%%\r\n\\Pr(&\\text{Miss, out of bounds}\\mid r,\\theta,\\kappa,\\delta,\\nu)\\\\\r\n&=(1-\\Pr(\\text{Miss, didn't make the distance}\\mid r,\\theta,\\kappa,\\delta,\\nu))(1-\\Pr(\\text{Goal}\\mid r,\\theta,\\kappa,\\delta,\\nu)-\\Pr(\\text{Behind} \\mid  r,\\theta,\\kappa,\\delta,\\nu))\r\n\\end{aligned}\r\n\\] where \\(t(x,\\nu)\\) is the cdf of Student’s \\(t\\) distribution with \\(\\nu\\) degrees of freedom, \\(r\\) and \\(\\theta\\) measure distance to goal and angle to goal, respectively, and \\(\\delta\\) is the parameter governing the distribution of kicking distance. \\(a(pq)\\) is the angle \\(pxq\\) (i.e. the angle between the kicker and the left two posts). I derived the expressions for the probabilities of goals and behinds, conditional on the kick making the distance, in this previous post\r\nLet’s assume that a kick has range \\(D\\sim\\mathrm{lognormal}(\\delta_1,\\delta_2)\\), and so now we have the following model in Stan:\r\n\r\n// saved as AFLAngleT.stan\r\ndata {\r\n  vector[2] priork;\r\n  vector[2] priornu;\r\n  vector[3] priorDistance;\r\n  int n;\r\n  vector[n] goal;\r\n  vector[n] behind;\r\n  vector[n] MissShort;\r\n  vector[n] bounds;\r\n  vector[n] aGoal;\r\n  vector[n] aLBehind;\r\n  vector[n] aRBehind;\r\n  vector[n] dist;\r\n}\r\ntransformed data {\r\n    vector[n] ones;\r\n    for (ii in 1:n) {\r\n        ones[ii] = 1;\r\n    }\r\n}\r\nparameters {\r\n  real<lower=0> k;     \r\n  real<lower=0> nu;\r\n  real ldm;\r\n  real<lower=0> ldsd;\r\n}\r\ntransformed parameters {\r\n  \r\n}\r\nmodel {\r\n    vector[n] pGoal;\r\n    vector[n] pLBehind;\r\n    vector[n] pRBehind;\r\n    vector[n] pDistance;\r\n    \r\n    \r\n    // prior\r\n    k ~ lognormal(priork[1],priork[2]);\r\n    nu ~ lognormal(priornu[1],priornu[2]);\r\n    ldm ~ normal(priorDistance[1],priorDistance[2]);\r\n    ldsd ~ exponential(priorDistance[3]);\r\n    \r\n\r\n    \r\n    // student_t_cdf(reals y, reals nu, reals mu, reals sigma)\r\n    for (ii in 1:n) {\r\n        pGoal[ii]       = student_t_cdf(aGoal[ii] / (2.0),nu,0.0,k)-student_t_cdf(-aGoal[ii] / (2.0),nu,0.0,k);\r\n        pLBehind[ii]    = student_t_cdf(-aGoal[ii]/(2.0),nu,0.0,k)-student_t_cdf(-aGoal[ii]/(2.0)-aLBehind[ii],nu,0.0,k);\r\n        pRBehind[ii]    = student_t_cdf(aRBehind[ii]+aGoal[ii]/(2.0),nu,0.0,k)-student_t_cdf(aGoal[ii]/(2.0),nu,0.0,k);\r\n        pDistance[ii] = normal_cdf(log(dist[ii]),ldm,ldsd);\r\n    }\r\n    \r\n    \r\n    \r\n    // likelihood contribution for goals\r\n    target += goal .* log(pGoal .* (1.0-pDistance));\r\n    // likelihood contribution for behinds\r\n    target += behind .* log((pLBehind+pRBehind)  .* (1.0-pDistance));\r\n    // likelihood contribution for miss, didn't make the distance\r\n    target += MissShort .* log(pDistance);\r\n    target += bounds .* log((1.0-pDistance) .* (1.0-pGoal - pRBehind-pLBehind));\r\n    \r\n    // old code for G/B/other\r\n    // likelihood contribution for goals\r\n    //target += goal .* log(pGoal);\r\n    // likelihood contribution for behinds\r\n    //target += behind .* log((pLBehind+pRBehind));\r\n    // likelihood contribution for the rest\r\n    //target += (MissShort+bounds) .* log(1.0-pLBehind-pRBehind-pGoal);\r\n}\r\n\r\ngenerated quantities {\r\n    \r\n}\r\n\r\nwhich produces the following results:\r\n\r\n\r\nmean\r\nse_mean\r\nsd\r\n2.5%\r\n97.5%\r\nn_eff\r\nRhat\r\nk\r\n0.0698231\r\n0.0000245\r\n0.0010919\r\n0.0677675\r\n0.0720713\r\n1989.142\r\n1.0002178\r\nnu\r\n7.4749125\r\n0.0184609\r\n0.7956717\r\n6.1443187\r\n9.2176177\r\n1857.633\r\n1.0003710\r\nldm\r\n4.3545964\r\n0.0006638\r\n0.0285137\r\n4.3014195\r\n4.4136017\r\n1844.933\r\n1.0009109\r\nldsd\r\n0.4727015\r\n0.0005644\r\n0.0238907\r\n0.4287772\r\n0.5233659\r\n1791.730\r\n1.0012361\r\nlp__\r\n-9832.0675061\r\n0.0382011\r\n1.4219847\r\n-9835.5459986\r\n-9830.3094180\r\n1385.598\r\n0.9999962\r\n\r\nThe parameters added since my previous post on this are “ldm” and “ldsd”, which govern the distribution of \\(D\\), kicking distance. Let’s have a look at what this implies about kicking range:\r\n\r\n\r\n\r\nSo now let’s have a look at what this means for the expected vale of shots from particular positions:\r\n\r\n\r\n\r\n\r\n\r\n\r\nIn this plot, the color shows the expected value of a set shot. The dots show empirical values (i.e. taking the average points scored from each position), and the contours show the model’s predictions. That is, shots along each contour have the same expected value. places where the contours and dots have similar colors are where the model is doing well.\r\nI will leave this here for now, but I am certainly not done with this dataset!\r\nBTW, I am going to be writing all of my posts here from now on, but if you want to see my older posts, they can be found on my google site\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-02-20-afldistance/afldistance_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-09-20T08:31:23-04:00",
    "input_file": {}
  }
]
